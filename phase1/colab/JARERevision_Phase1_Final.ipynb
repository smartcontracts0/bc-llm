{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zUAm0PBa5FQ"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"transformers==4.55.0\" \"tokenizers==0.21.4\" \"pandas==2.2.2\" \"eth-hash[pycryptodome]==0.5.2\" \"requests==2.32.3\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, gzip, shutil, json, time, zipfile\n",
        "from pathlib import Path\n",
        "import requests, pandas as pd\n",
        "from eth_hash.auto import keccak\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Config (match your paper)\n",
        "ZIP_URL   = \"https://physionet.org/static/published-projects/mimic-iv-demo/mimic-iv-clinical-database-demo-2.2.zip\"\n",
        "ROW_LIMIT = 200\n",
        "MODEL_REPO = \"Qwen/Qwen2.5-3B-Instruct\"   # tokenizer only (no model)\n",
        "TARGET_FILE = \"tokenized_admissions.jsonl\"  # change if you want a different proof target\n",
        "\n",
        "RAW_DIR   = Path(\"raw_data\")\n",
        "CANON_DIR = Path(\"canonical_data\")\n",
        "TOK_DIR   = Path(\"tokenized_data\")\n",
        "RAW_DIR.mkdir(exist_ok=True); CANON_DIR.mkdir(exist_ok=True); TOK_DIR.mkdir(exist_ok=True)\n"
      ],
      "metadata": {
        "id": "mHj-flbMIDCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from eth_hash.auto import keccak\n",
        "\n",
        "# --- Download ZIP once ---\n",
        "zip_path = Path(\"mimic_demo.zip\")\n",
        "if not zip_path.exists():\n",
        "    r = requests.get(ZIP_URL, stream=True); r.raise_for_status()\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(8192): f.write(chunk)\n",
        "\n",
        "# --- Extract ZIP ---\n",
        "if not any(RAW_DIR.iterdir()):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(RAW_DIR)\n",
        "\n",
        "# --- Canonicalize helpers (match paper) ---\n",
        "def canonicalize_csv(in_path, out_path):\n",
        "    try:\n",
        "        df = pd.read_csv(in_path, dtype=str)\n",
        "    except Exception as e:\n",
        "        print(\"skip:\", in_path.name, e); return False\n",
        "    cols = sorted(df.columns)\n",
        "    df = df[cols].sort_values(by=cols, na_position=\"last\").reset_index(drop=True)\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8\", lineterminator=\"\\n\")\n",
        "    return True\n",
        "\n",
        "# --- Decompress .csv.gz then canonicalize all CSVs (except demo_subject_id) ---\n",
        "for fp in sorted(RAW_DIR.rglob(\"*\")):\n",
        "    if not fp.is_file():\n",
        "        continue\n",
        "    if \"demo_subject_id\" in fp.name.lower():\n",
        "        continue\n",
        "    if fp.suffix == \".gz\" and fp.name.endswith(\".csv.gz\"):\n",
        "        dec = fp.with_suffix(\"\")  # drop .gz\n",
        "        with gzip.open(fp, \"rb\") as fin, open(dec, \"wb\") as fout:\n",
        "            shutil.copyfileobj(fin, fout)\n",
        "        fp = dec\n",
        "    if fp.suffix.lower() == \".csv\":\n",
        "        out = CANON_DIR / fp.name\n",
        "        if canonicalize_csv(fp, out):\n",
        "            print(\"canonicalized:\", fp.name)\n",
        "\n",
        "# --- Tokenize (Qwen tokenizer) ---\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_REPO)  # uses HF hub\n",
        "def build_records(csv_path):\n",
        "    df = pd.read_csv(csv_path, dtype=str).head(ROW_LIMIT)\n",
        "    prompts, responses = [], []\n",
        "    for _, row in df.iterrows():\n",
        "        # Python dict repr (single quotes; NaN -> nan)\n",
        "        prompts.append(\n",
        "            f\"You are a clinical assistant. Given the following record from {csv_path.name}, \"\n",
        "            f\"provide a short summary:\\n{row.to_dict()}\"\n",
        "        )\n",
        "        responses.append(\"Summary: [Your summary here]\")\n",
        "    enc = tok(prompts, text_pair=responses, truncation=True, max_length=512)\n",
        "    out_path = TOK_DIR / f\"tokenized_{csv_path.stem}.jsonl\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(prompts)):\n",
        "            rec = {\n",
        "                \"prompt\": prompts[i],\n",
        "                \"response\": responses[i],\n",
        "                \"input_ids\": list(map(int, enc[\"input_ids\"][i])),\n",
        "                \"attention_mask\": list(map(int, enc[\"attention_mask\"][i])),\n",
        "            }\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "    return out_path\n",
        "\n",
        "TOK_DIR.mkdir(exist_ok=True)\n",
        "for csv in sorted(CANON_DIR.glob(\"*.csv\")):\n",
        "    out = build_records(csv); print(\"tokenized ->\", out.name)\n",
        "\n",
        "# --- Keccak(file bytes) for each JSONL ---\n",
        "def file_keccak(p: Path):\n",
        "    with open(p, \"rb\") as f:\n",
        "        return keccak(f.read())\n",
        "\n",
        "files  = sorted([p for p in TOK_DIR.iterdir() if p.suffix == \".jsonl\"], key=lambda p: p.name)\n",
        "leaves = [file_keccak(p) for p in files]\n",
        "\n",
        "# === Merkle (SORTED-PAIR rule; duplicate last when odd) ===\n",
        "def _parent(a: bytes, b: bytes) -> bytes:\n",
        "    # match Solidity: if (a < b) hash(a||b) else hash(b||a)\n",
        "    return keccak(a + b) if a < b else keccak(b + a)\n",
        "\n",
        "def merkle_root(hashes: list[bytes]) -> bytes:\n",
        "    level = hashes[:]\n",
        "    if not level:\n",
        "        raise ValueError(\"no leaves\")\n",
        "    while len(level) > 1:\n",
        "        if len(level) % 2 == 1:\n",
        "            level.append(level[-1])\n",
        "        nxt = []\n",
        "        for i in range(0, len(level), 2):\n",
        "            nxt.append(_parent(level[i], level[i+1]))\n",
        "        level = nxt\n",
        "    return level[0]\n",
        "\n",
        "def merkle_proof(hashes: list[bytes], idx: int) -> list[str]:\n",
        "    proof, level_i, level = [], idx, hashes[:]\n",
        "    if not (0 <= idx < len(level)):\n",
        "        raise IndexError(\"target index out of range\")\n",
        "    while len(level) > 1:\n",
        "        if len(level) % 2 == 1:\n",
        "            level.append(level[-1])\n",
        "        nxt = []\n",
        "        for i in range(0, len(level), 2):\n",
        "            L, R = level[i], level[i+1]\n",
        "            if i == level_i or i+1 == level_i:\n",
        "                sib = R if i == level_i else L\n",
        "                proof.append(\"0x\" + sib.hex())     # sibling only; no direction needed\n",
        "                level_i = len(nxt)\n",
        "            nxt.append(_parent(L, R))\n",
        "        level = nxt\n",
        "    return proof\n",
        "\n",
        "# --- Root / proof for TARGET_FILE ---\n",
        "root_bytes = merkle_root(leaves)\n",
        "root       = \"0x\" + root_bytes.hex()\n",
        "\n",
        "tgt_i      = next(i for i, f in enumerate(files) if f.name == TARGET_FILE)\n",
        "proof      = merkle_proof(leaves, tgt_i)\n",
        "leaf_hex   = \"0x\" + leaves[tgt_i].hex()\n",
        "\n",
        "# --- Write artifacts for the paper & registration ---\n",
        "with open(\"merkle_hashes.txt\", \"w\") as f:\n",
        "    f.write(f\"Merkle Root: {root}\\n\\n\")\n",
        "    for p, h in zip(files, leaves):\n",
        "        f.write(f\"{p.name}: 0x{h.hex()}\\n\")\n",
        "    f.write(\"\\nProof for \" + TARGET_FILE + \":\\n\")\n",
        "    f.write(\"Leaf Hash: \" + leaf_hex + \"\\n\")\n",
        "    f.write(\"Proof Array: \" + str(proof) + \"\\n\")\n",
        "\n",
        "# Build metadata JSON for the dapp to upload to IPFS\n",
        "import platform, time, json\n",
        "# DATASET_NAME already defined elsewhere in your notebook; if not, set it:\n",
        "DATASET_NAME = \"MIMIC-IV-Demo-v2.2\"\n",
        "dataset_id_hex = \"0x\" + keccak(DATASET_NAME.encode(\"utf-8\")).hex()\n",
        "\n",
        "# Use the leaf hashes we already computed\n",
        "file_hash_map = {p.name: \"0x\" + h.hex() for p, h in zip(files, leaves)}\n",
        "total_size = sum(p.stat().st_size for p in files)\n",
        "files_info = [{\n",
        "    \"filename\": p.name,\n",
        "    \"size_bytes\": p.stat().st_size,\n",
        "    \"keccak256\": file_hash_map[p.name],\n",
        "} for p in files]\n",
        "\n",
        "preprocessing_metadata = {\n",
        "    \"dataset\": {\n",
        "        \"name\": DATASET_NAME,\n",
        "        \"dataset_id\": dataset_id_hex,\n",
        "        \"description\": \"Tokenized clinical dataset prepared for Qwen2.5-3B-Instruct fine-tuning.\",\n",
        "        \"merkle_root\": root,\n",
        "        \"num_files\": len(files_info),\n",
        "        \"total_size_bytes\": total_size,\n",
        "        \"files\": files_info,\n",
        "        \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "    },\n",
        "    \"tokenizer\": { \"name\": MODEL_REPO },\n",
        "    \"prompt_template\": {\n",
        "        \"description\": \"You are a clinical assistant. Given the following record from {filename}, provide a short summary.\",\n",
        "        \"max_length\": 512,\n",
        "        \"truncation\": True\n",
        "    },\n",
        "    \"environment\": {\n",
        "        \"os\": platform.system(),\n",
        "        \"python_version\": platform.python_version(),\n",
        "        \"pandas_version\": pd.__version__\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"preprocessing_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(preprocessing_metadata, f, indent=2)\n",
        "\n",
        "# Optional: write a viewer-friendly verify bundle (no version yet)\n",
        "verify_bundle = {\n",
        "    \"datasetId\": dataset_id_hex,\n",
        "    \"merkleRoot\": root,\n",
        "    \"targetFile\": TARGET_FILE,\n",
        "    \"leafHash\": leaf_hex,\n",
        "    \"proof\": proof\n",
        "}\n",
        "with open(\"verify_bundle.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(verify_bundle, f, indent=2)\n",
        "\n",
        "print(\"\\n=== RESULTS ===\")\n",
        "print(\"Root:\", root)\n",
        "print(\"Target leaf:\", leaf_hex)\n",
        "print(\"Proof:\", proof)\n",
        "print(\"\\nWrote: merkle_hashes.txt, preprocessing_metadata.json, verify_bundle.json, tokenized_data/\")\n"
      ],
      "metadata": {
        "id": "2-4JZVxOa_31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Create a lightweight verify bundle for the dapp ===\n",
        "import json, time\n",
        "\n",
        "DS_VERSION = 1  # <-- set this to the version you want to verify\n",
        "\n",
        "verify_bundle = {\n",
        "    \"datasetId\": dataset_id_hex,   # 0x… bytes32\n",
        "    \"version\": DS_VERSION,         # uint256\n",
        "    \"leafHash\": leaf_hex,          # 0x… bytes32 (target file’s keccak256)\n",
        "    \"proof\": proof,                # [\"0x…\", \"0x…\", ...]\n",
        "    \"merkleRoot\": root,            # optional, for display/debug\n",
        "    \"targetFile\": TARGET_FILE,     # optional\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "}\n",
        "with open(\"verify_bundle.json\",\"w\") as f:\n",
        "    json.dump(verify_bundle, f, indent=2)\n",
        "\n",
        "print(\"Wrote verify_bundle.json\")\n"
      ],
      "metadata": {
        "id": "HYK9aKOO7HXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R0-TtP411C7s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}