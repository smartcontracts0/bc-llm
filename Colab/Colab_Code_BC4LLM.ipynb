{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNg1D8SPWJir"
      },
      "source": [
        "# Phase1: Dataset Registration and Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SU0GuY9FWJ0T"
      },
      "outputs": [],
      "source": [
        "!pip -q install \"transformers==4.55.0\" \"tokenizers==0.21.4\" \"pandas==2.2.2\" \"eth-hash[pycryptodome]==0.5.2\" \"requests==2.32.3\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zg8HPdcB0YxE"
      },
      "outputs": [],
      "source": [
        "# --- Extra installs for Web3 interaction ---\n",
        "!pip install web3==6.19.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOouhllXBsuI"
      },
      "source": [
        "**Phase1 Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHj-flbMIDCR"
      },
      "outputs": [],
      "source": [
        "import os, gzip, shutil, json, time, zipfile\n",
        "from pathlib import Path\n",
        "import requests, pandas as pd\n",
        "from eth_hash.auto import keccak\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Config (match your paper)\n",
        "ZIP_URL   = \"https://physionet.org/static/published-projects/mimic-iv-demo/mimic-iv-clinical-database-demo-2.2.zip\"\n",
        "ROW_LIMIT = 200\n",
        "MODEL_REPO = \"Qwen/Qwen2.5-3B-Instruct\"   # tokenizer only (no model)\n",
        "TARGET_FILE = \"tokenized_admissions.jsonl\"  # change if you want a different proof target\n",
        "\n",
        "RAW_DIR   = Path(\"raw_data\")\n",
        "CANON_DIR = Path(\"canonical_data\")\n",
        "TOK_DIR   = Path(\"tokenized_data\")\n",
        "RAW_DIR.mkdir(exist_ok=True); CANON_DIR.mkdir(exist_ok=True); TOK_DIR.mkdir(exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S-pFtED1oia"
      },
      "outputs": [],
      "source": [
        "from urllib.parse import urlparse\n",
        "from web3 import Web3\n",
        "\n",
        "def infer_dataset_name(zip_url: str) -> str:\n",
        "    fn = Path(urlparse(zip_url).path).name  # e.g., mimic-iv-clinical-database-demo-2.2.zip\n",
        "    return fn[:-4] if fn.lower().endswith(\".zip\") else fn\n",
        "\n",
        "# Use explicit name if you set DATASET_NAME; otherwise infer from ZIP_URL filename.\n",
        "DATASET_NAME = globals().get(\"DATASET_NAME\") or infer_dataset_name(ZIP_URL)\n",
        "\n",
        "# Phase-1 computeDatasetId(string) == keccak256(abi.encodePacked(name))\n",
        "dataset_id_bytes = Web3.keccak(text=DATASET_NAME)\n",
        "dataset_id_hex   = \"0x\" + dataset_id_bytes.hex()\n",
        "\n",
        "# Optional alias so older cells using the uppercase name still work\n",
        "DATASET_ID_HEX = dataset_id_hex\n",
        "\n",
        "# --- fix double 0x / whitespace ---\n",
        "DATASET_ID_HEX = DATASET_ID_HEX.strip()\n",
        "while DATASET_ID_HEX.lower().startswith(\"0x0x\"):\n",
        "    DATASET_ID_HEX = DATASET_ID_HEX[2:]\n",
        "\n",
        "print(\"DATASET_NAME:\", DATASET_NAME)\n",
        "print(\"DATASET_ID_HEX:\", DATASET_ID_HEX)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-4JZVxOa_31"
      },
      "outputs": [],
      "source": [
        "from eth_hash.auto import keccak\n",
        "\n",
        "# --- Download ZIP once ---\n",
        "zip_path = Path(\"mimic_demo.zip\")\n",
        "if not zip_path.exists():\n",
        "    r = requests.get(ZIP_URL, stream=True); r.raise_for_status()\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(8192): f.write(chunk)\n",
        "\n",
        "# --- Extract ZIP ---\n",
        "if not any(RAW_DIR.iterdir()):\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(RAW_DIR)\n",
        "\n",
        "# --- Canonicalize helpers (match paper) ---\n",
        "def canonicalize_csv(in_path, out_path):\n",
        "    try:\n",
        "        df = pd.read_csv(in_path, dtype=str)\n",
        "    except Exception as e:\n",
        "        print(\"skip:\", in_path.name, e); return False\n",
        "    cols = sorted(df.columns)\n",
        "    df = df[cols].sort_values(by=cols, na_position=\"last\").reset_index(drop=True)\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8\", lineterminator=\"\\n\")\n",
        "    return True\n",
        "\n",
        "# --- Decompress .csv.gz then canonicalize all CSVs (except demo_subject_id) ---\n",
        "for fp in sorted(RAW_DIR.rglob(\"*\")):\n",
        "    if not fp.is_file():\n",
        "        continue\n",
        "    if \"demo_subject_id\" in fp.name.lower():\n",
        "        continue\n",
        "    if fp.suffix == \".gz\" and fp.name.endswith(\".csv.gz\"):\n",
        "        dec = fp.with_suffix(\"\")  # drop .gz\n",
        "        with gzip.open(fp, \"rb\") as fin, open(dec, \"wb\") as fout:\n",
        "            shutil.copyfileobj(fin, fout)\n",
        "        fp = dec\n",
        "    if fp.suffix.lower() == \".csv\":\n",
        "        out = CANON_DIR / fp.name\n",
        "        if canonicalize_csv(fp, out):\n",
        "            print(\"canonicalized:\", fp.name)\n",
        "\n",
        "# --- Tokenize (Qwen tokenizer) ---\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_REPO)  # uses HF hub\n",
        "def build_records(csv_path):\n",
        "    df = pd.read_csv(csv_path, dtype=str).head(ROW_LIMIT)\n",
        "    prompts, responses = [], []\n",
        "    for _, row in df.iterrows():\n",
        "        # Python dict repr (single quotes; NaN -> nan)\n",
        "        prompts.append(\n",
        "            f\"You are a clinical assistant. Given the following record from {csv_path.name}, \"\n",
        "            f\"provide a short summary:\\n{row.to_dict()}\"\n",
        "        )\n",
        "        responses.append(\"Summary: [Your summary here]\")\n",
        "    enc = tok(prompts, text_pair=responses, truncation=True, max_length=512)\n",
        "    out_path = TOK_DIR / f\"tokenized_{csv_path.stem}.jsonl\"\n",
        "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for i in range(len(prompts)):\n",
        "            rec = {\n",
        "                \"prompt\": prompts[i],\n",
        "                \"response\": responses[i],\n",
        "                \"input_ids\": list(map(int, enc[\"input_ids\"][i])),\n",
        "                \"attention_mask\": list(map(int, enc[\"attention_mask\"][i])),\n",
        "            }\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "    return out_path\n",
        "\n",
        "TOK_DIR.mkdir(exist_ok=True)\n",
        "for csv in sorted(CANON_DIR.glob(\"*.csv\")):\n",
        "    out = build_records(csv); print(\"tokenized ->\", out.name)\n",
        "\n",
        "# --- Keccak(file bytes) for each JSONL ---\n",
        "def file_keccak(p: Path):\n",
        "    with open(p, \"rb\") as f:\n",
        "        return keccak(f.read())\n",
        "\n",
        "files  = sorted([p for p in TOK_DIR.iterdir() if p.suffix == \".jsonl\"], key=lambda p: p.name)\n",
        "leaves = [file_keccak(p) for p in files]\n",
        "\n",
        "# === Merkle (SORTED-PAIR rule; duplicate last when odd) ===\n",
        "def _parent(a: bytes, b: bytes) -> bytes:\n",
        "    # match Solidity: if (a < b) hash(a||b) else hash(b||a)\n",
        "    return keccak(a + b) if a < b else keccak(b + a)\n",
        "\n",
        "def merkle_root(hashes: list[bytes]) -> bytes:\n",
        "    level = hashes[:]\n",
        "    if not level:\n",
        "        raise ValueError(\"no leaves\")\n",
        "    while len(level) > 1:\n",
        "        if len(level) % 2 == 1:\n",
        "            level.append(level[-1])\n",
        "        nxt = []\n",
        "        for i in range(0, len(level), 2):\n",
        "            nxt.append(_parent(level[i], level[i+1]))\n",
        "        level = nxt\n",
        "    return level[0]\n",
        "\n",
        "def merkle_proof(hashes: list[bytes], idx: int) -> list[str]:\n",
        "    proof, level_i, level = [], idx, hashes[:]\n",
        "    if not (0 <= idx < len(level)):\n",
        "        raise IndexError(\"target index out of range\")\n",
        "    while len(level) > 1:\n",
        "        if len(level) % 2 == 1:\n",
        "            level.append(level[-1])\n",
        "        nxt = []\n",
        "        for i in range(0, len(level), 2):\n",
        "            L, R = level[i], level[i+1]\n",
        "            if i == level_i or i+1 == level_i:\n",
        "                sib = R if i == level_i else L\n",
        "                proof.append(\"0x\" + sib.hex())     # sibling only; no direction needed\n",
        "                level_i = len(nxt)\n",
        "            nxt.append(_parent(L, R))\n",
        "        level = nxt\n",
        "    return proof\n",
        "\n",
        "# --- Root / proof for TARGET_FILE ---\n",
        "root_bytes = merkle_root(leaves)\n",
        "root       = \"0x\" + root_bytes.hex()\n",
        "\n",
        "tgt_i      = next(i for i, f in enumerate(files) if f.name == TARGET_FILE)\n",
        "proof      = merkle_proof(leaves, tgt_i)\n",
        "leaf_hex   = \"0x\" + leaves[tgt_i].hex()\n",
        "\n",
        "# --- Write artifacts for the paper & registration ---\n",
        "with open(\"merkle_hashes.txt\", \"w\") as f:\n",
        "    f.write(f\"Merkle Root: {root}\\n\\n\")\n",
        "    for p, h in zip(files, leaves):\n",
        "        f.write(f\"{p.name}: 0x{h.hex()}\\n\")\n",
        "    f.write(\"\\nProof for \" + TARGET_FILE + \":\\n\")\n",
        "    f.write(\"Leaf Hash: \" + leaf_hex + \"\\n\")\n",
        "    f.write(\"Proof Array: \" + str(proof) + \"\\n\")\n",
        "\n",
        "# Build metadata JSON for the dapp to upload to IPFS\n",
        "import platform, time, json\n",
        "# DATASET_NAME already defined elsewhere in your notebook; if not, set it:\n",
        "DATASET_NAME = \"MIMIC-IV-Demo-v2.2\"\n",
        "dataset_id_hex = \"0x\" + keccak(DATASET_NAME.encode(\"utf-8\")).hex()\n",
        "\n",
        "# Use the leaf hashes we already computed\n",
        "file_hash_map = {p.name: \"0x\" + h.hex() for p, h in zip(files, leaves)}\n",
        "total_size = sum(p.stat().st_size for p in files)\n",
        "files_info = [{\n",
        "    \"filename\": p.name,\n",
        "    \"size_bytes\": p.stat().st_size,\n",
        "    \"keccak256\": file_hash_map[p.name],\n",
        "} for p in files]\n",
        "\n",
        "preprocessing_metadata = {\n",
        "    \"dataset\": {\n",
        "        \"name\": DATASET_NAME,\n",
        "        \"dataset_id\": dataset_id_hex,\n",
        "        \"description\": \"Tokenized clinical dataset prepared for Qwen2.5-3B-Instruct fine-tuning.\",\n",
        "        \"merkle_root\": root,\n",
        "        \"num_files\": len(files_info),\n",
        "        \"total_size_bytes\": total_size,\n",
        "        \"files\": files_info,\n",
        "        \"created_at\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime())\n",
        "    },\n",
        "    \"tokenizer\": { \"name\": MODEL_REPO },\n",
        "    \"prompt_template\": {\n",
        "        \"description\": \"You are a clinical assistant. Given the following record from {filename}, provide a short summary.\",\n",
        "        \"max_length\": 512,\n",
        "        \"truncation\": True\n",
        "    },\n",
        "    \"environment\": {\n",
        "        \"os\": platform.system(),\n",
        "        \"python_version\": platform.python_version(),\n",
        "        \"pandas_version\": pd.__version__\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"preprocessing_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(preprocessing_metadata, f, indent=2)\n",
        "\n",
        "# Optional: write a viewer-friendly verify bundle (no version yet)\n",
        "verify_bundle = {\n",
        "    \"datasetId\": dataset_id_hex,\n",
        "    \"merkleRoot\": root,\n",
        "    \"targetFile\": TARGET_FILE,\n",
        "    \"leafHash\": leaf_hex,\n",
        "    \"proof\": proof\n",
        "}\n",
        "with open(\"verify_bundle.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(verify_bundle, f, indent=2)\n",
        "\n",
        "print(\"\\n=== RESULTS ===\")\n",
        "print(\"Root:\", root)\n",
        "print(\"Target leaf:\", leaf_hex)\n",
        "print(\"Proof:\", proof)\n",
        "print(\"\\nWrote: merkle_hashes.txt, preprocessing_metadata.json, verify_bundle.json, tokenized_data/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYK9aKOO7HXw"
      },
      "outputs": [],
      "source": [
        "# === Create a lightweight verify bundle for the dapp (robust; no chain dependency) ===\n",
        "import os, json, time, string\n",
        "from web3 import Web3\n",
        "\n",
        "def _b32_hex(v: str) -> str:\n",
        "    s = (v or \"\").strip()\n",
        "    while s.lower().startswith(\"0x\"):  # strip ANY number of 0x\n",
        "        s = s[2:]\n",
        "    s = \"\".join(s.split())             # remove spaces/newlines\n",
        "    assert len(s) == 64 and all(c in string.hexdigits for c in s), f\"bad bytes32: {v!r}\"\n",
        "    return \"0x\" + s.lower()\n",
        "\n",
        "# Resolve datasetId: use provided hex or derive from a name\n",
        "if not globals().get(\"DATASET_ID_HEX\") or str(DATASET_ID_HEX).strip().lower() in (\"\", \"0x\", \"auto\"):\n",
        "    assert globals().get(\"DATASET_NAME\"), \"Set DATASET_NAME or provide DATASET_ID_HEX\"\n",
        "    DATASET_ID_HEX = \"0x\" + Web3.keccak(text=DATASET_NAME).hex()[2:]\n",
        "DATASET_ID_HEX = _b32_hex(DATASET_ID_HEX)\n",
        "\n",
        "# Resolve dataset version WITHOUT touching chain\n",
        "# - Prefer env/variable DATASET_VER if set, else default to 1 during local build.\n",
        "DS_VERSION = int(os.getenv(\"DATASET_VER\", str(globals().get(\"DATASET_VER\", 1))))\n",
        "\n",
        "ROW_LIMIT = 200  # disclosure: only first N rows were used\n",
        "\n",
        "verify_bundle = {\n",
        "    \"datasetId\": DATASET_ID_HEX,   # 0x… bytes32\n",
        "    \"version\": DS_VERSION,         # uint256\n",
        "    \"leafHash\": leaf_hex,          # 0x… bytes32 (target file’s keccak256)\n",
        "    \"proof\": proof,                # [\"0x…\", ...]\n",
        "    \"merkleRoot\": root,            # 0x… bytes32 (optional)\n",
        "    \"targetFile\": TARGET_FILE,     # optional\n",
        "    \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
        "    \"methodology\": {\n",
        "        \"row_limit\": ROW_LIMIT,\n",
        "        \"note\": f\"Only the first {ROW_LIMIT} rows of each CSV were used during tokenization \"\n",
        "                \"to create this dataset for demonstration purposes.\"\n",
        "    }\n",
        "}\n",
        "\n",
        "with open(\"verify_bundle.json\", \"w\") as f:\n",
        "    json.dump(verify_bundle, f, indent=2)\n",
        "\n",
        "print(\"Wrote verify_bundle.json with disclosure\")\n",
        "print(\"datasetId:\", DATASET_ID_HEX, \"version:\", DS_VERSION)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoGPkrXYK0I7"
      },
      "source": [
        "# Phase1: Dataset Registration and Verification [Connect to SC]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIURq1RwzvIr"
      },
      "source": [
        "**This step send the tx. Otherwise, it can be done via the UI**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01rZs3GLK3p1"
      },
      "outputs": [],
      "source": [
        "from web3 import Web3\n",
        "import json\n",
        "\n",
        "# ==== CONFIG ====\n",
        "RPC_URL = \"Replace_with_your_URL\"\n",
        "PRIVATE_KEY = \"Replace_with_you_PK\"  # your wallet PK (testnet)\n",
        "ACCOUNT = Web3.to_checksum_address(Web3().eth.account.from_key(PRIVATE_KEY).address)\n",
        "REGISTRY_ADDR = Web3.to_checksum_address(\"DataRegistryAddress\")  # deployed DatasetRegistry\n",
        "\n",
        "# Load registry ABI (from your Solidity build)\n",
        "registry_abi = json.loads(\"\"\"<add_abi>\"\"\")\n",
        "\n",
        "w3 = Web3(Web3.HTTPProvider(RPC_URL))\n",
        "assert w3.is_connected(), \"Web3 not connected\"\n",
        "\n",
        "registry = w3.eth.contract(address=REGISTRY_ADDR, abi=registry_abi)\n",
        "\n",
        "# === Pin preprocessing_metadata.json to IPFS ===\n",
        "PINATA_JWT = \"Replace_with_your_JWT\"\n",
        "pinata_url = \"https://api.pinata.cloud/pinning/pinFileToIPFS\"\n",
        "\n",
        "# Load and update preprocessing metadata\n",
        "with open(\"preprocessing_metadata.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    preprocessing_meta = json.load(f)\n",
        "\n",
        "# Add disclosure of the row limit\n",
        "ROW_LIMIT = 200\n",
        "preprocessing_meta[\"methodology\"] = {\n",
        "    \"row_limit\": ROW_LIMIT,\n",
        "    \"note\": f\"Only the first {ROW_LIMIT} rows of each CSV were used during tokenization \"\n",
        "            \"to create this dataset for demonstration purposes.\"\n",
        "}\n",
        "\n",
        "# Overwrite local file so we pin the updated one\n",
        "with open(\"preprocessing_metadata.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(preprocessing_meta, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Pin to IPFS via Pinata\n",
        "with open(\"preprocessing_metadata.json\", \"rb\") as f:\n",
        "    files = {\"file\": (\"preprocessing_metadata.json\", f)}\n",
        "    headers = {\"Authorization\": f\"Bearer {PINATA_JWT}\"}\n",
        "    resp = requests.post(pinata_url, files=files, headers=headers)\n",
        "    resp.raise_for_status()\n",
        "    metadata_cid = resp.json()[\"IpfsHash\"]\n",
        "\n",
        "print(\"Pinned metadata CID:\", metadata_cid)\n",
        "\n",
        "# === Register dataset on-chain ===\n",
        "dataset_id_bytes32 = Web3.to_bytes(hexstr=dataset_id_hex)\n",
        "merkle_root_bytes32 = Web3.to_bytes(hexstr=root)\n",
        "\n",
        "nonce = w3.eth.get_transaction_count(ACCOUNT)\n",
        "tx = registry.functions.registerDataset(\n",
        "    dataset_id_bytes32,\n",
        "    merkle_root_bytes32,\n",
        "    f\"ipfs://{metadata_cid}\"\n",
        ").build_transaction({\n",
        "    \"from\": ACCOUNT,\n",
        "    \"nonce\": nonce,\n",
        "    \"gas\": 300000,\n",
        "    \"gasPrice\": w3.to_wei(\"2\", \"gwei\")\n",
        "})\n",
        "\n",
        "signed = w3.eth.account.sign_transaction(tx, PRIVATE_KEY)\n",
        "tx_hash = w3.eth.send_raw_transaction(signed.rawTransaction)\n",
        "print(\"registerDataset tx sent:\", tx_hash.hex())\n",
        "receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n",
        "print(\"Status:\", receipt.status)\n",
        "\n",
        "# === Verify file membership ===\n",
        "leaf_bytes32 = Web3.to_bytes(hexstr=leaf_hex)\n",
        "proof_bytes = [Web3.to_bytes(hexstr=p) for p in proof]\n",
        "\n",
        "is_member = registry.functions.verifyFileHash(\n",
        "    dataset_id_bytes32,\n",
        "    DS_VERSION,\n",
        "    leaf_bytes32,\n",
        "    proof_bytes\n",
        ").call()\n",
        "\n",
        "print(f\"File {TARGET_FILE} membership verified on-chain:\", is_member)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byEI0T1lPqO2"
      },
      "source": [
        "**[Extra Step]: Verification and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAV5Sa6zPvqa"
      },
      "outputs": [],
      "source": [
        "# === REBUILD CHECK: Ensure Merkle root matches on-chain ===\n",
        "from eth_utils import keccak\n",
        "\n",
        "def file_keccak(p):\n",
        "    with open(p, \"rb\") as f:\n",
        "        return keccak(f.read())\n",
        "\n",
        "files  = sorted([p for p in TOK_DIR.iterdir() if p.suffix == \".jsonl\"], key=lambda p: p.name)\n",
        "leaves = [file_keccak(p) for p in files]\n",
        "\n",
        "# Merkle build (sorted pairs, same as contract)\n",
        "def merkle_root(leaves):\n",
        "    if len(leaves) == 1:\n",
        "        return leaves[0]\n",
        "    if len(leaves) % 2 == 1:\n",
        "        leaves.append(leaves[-1])\n",
        "    new_level = []\n",
        "    for i in range(0, len(leaves), 2):\n",
        "        pair = sorted([leaves[i], leaves[i+1]])\n",
        "        new_level.append(keccak(pair[0] + pair[1]))\n",
        "    return merkle_root(new_level)\n",
        "\n",
        "local_root = \"0x\" + merkle_root(leaves).hex()\n",
        "print(\"Local root:   \", local_root)\n",
        "print(\"On-chain root:\", root)\n",
        "assert local_root.lower() == root.lower(), \"Mismatch! Something changed.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaR_3e5ZPw2M"
      },
      "outputs": [],
      "source": [
        "# === NEGATIVE TEST: Ensure wrong data fails verification ===\n",
        "import os\n",
        "fake_leaf = os.urandom(32)  # random bytes, not in dataset\n",
        "fake_proof = proof  # any valid proof\n",
        "\n",
        "is_member_fake = registry.functions.verifyFileHash(\n",
        "    dataset_id_bytes32,\n",
        "    DS_VERSION,\n",
        "    fake_leaf,\n",
        "    fake_proof\n",
        ").call()\n",
        "\n",
        "print(\"Expected False, got:\", is_member_fake)\n",
        "assert not is_member_fake, \"Verifier incorrectly accepted wrong file!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o213pY9UPw41"
      },
      "outputs": [],
      "source": [
        "# === DATASET ID CROSS-CHECK ===\n",
        "calc_onchain = registry.functions.computeDatasetId(DATASET_NAME).call()\n",
        "calc_local   = keccak(DATASET_NAME.encode())\n",
        "print(\"On-chain computeDatasetId:\", calc_onchain.hex())\n",
        "print(\"Local keccak of name:     \", calc_local.hex())\n",
        "assert calc_onchain == calc_local, \"Dataset ID mismatch!\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PMZc_B9Pw9W"
      },
      "outputs": [],
      "source": [
        "# === PIN verify_bundle.json for third-party verification ===\n",
        "with open(\"verify_bundle.json\", \"rb\") as f:\n",
        "    files = {\"file\": (\"verify_bundle.json\", f)}\n",
        "    headers = {\"Authorization\": f\"Bearer {PINATA_JWT}\"}\n",
        "    resp = requests.post(pinata_url, files=files, headers=headers)\n",
        "    resp.raise_for_status()\n",
        "    verify_cid = resp.json()[\"IpfsHash\"]\n",
        "\n",
        "print(\"Verify bundle CID:\", verify_cid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgoaSEYW-vtV"
      },
      "source": [
        "# Phase2: Model Training and Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keaxBDTPCbh2"
      },
      "source": [
        "**Install packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5jnOSGMBBRH"
      },
      "outputs": [],
      "source": [
        "# If you just started a fresh Colab T4, run this ONCE.\n",
        "# Do NOT install custom torch/cuda or bitsandbytes here — it causes conflicts.\n",
        "%pip install -U \"transformers>=4.44,<4.47\" \"accelerate>=0.33,<0.36\" \"peft>=0.13,<0.14\" \\\n",
        "  \"datasets>=2.20,<2.22\" einops sentencepiece evaluate scikit-learn \\\n",
        "  web3==6.19.0 eth-account==0.10.0 requests\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMHKd0zmCaJB"
      },
      "source": [
        "**Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRhczWPnBE6Z"
      },
      "outputs": [],
      "source": [
        "#@title ✅ Edit these for your environment\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# --------- Chain RPC + Contracts ----------\n",
        "RPC_URL        = \"\"  #@param {type:\"string\"}\n",
        "PHASE1_ADDR    = \"\"                            #@param {type:\"string\"}\n",
        "PHASE2_ADDR    = \"\"                            #@param {type:\"string\"}\n",
        "PRIVATE_KEY    = \"\"                  #@param {type:\"string\"}  # Sepolia test key ONLY\n",
        "\n",
        "# ---------- Phase-1 Dataset identity ----------\n",
        "#here it is assumed that the dataset registered in phase1 is also used here\n",
        "DATASET_ID_HEX = \"\"               #@param {type:\"string\"}\n",
        "# 0 = auto (next cell will resolve from the registry events); or set an explicit integer\n",
        "DATASET_VER = int(os.getenv(\"DATASET_VER\", \"0\"))\n",
        "\n",
        "# ---------- IPFS / Pinning (optional) ----------\n",
        "PINATA_JWT     = \"\"                       #@param {type:\"string\"}\n",
        "W3S_TOKEN      = \"\"  # web3.storage API token                      #@param {type:\"string\"}\n",
        "IPFS_PROXY     = \"\"      #@param {type:\"string\"}\n",
        "\n",
        "# ---------- Data ----------\n",
        "ZIP_URL        = \"https://physionet.org/static/published-projects/mimic-iv-demo/mimic-iv-clinical-database-demo-2.2.zip\"  #@param {type:\"string\"}\n",
        "\n",
        "# ---------- Training ----------\n",
        "MODEL_REPO     = \"Qwen/Qwen2.5-0.5B-Instruct\"  #@param {type:\"string\"}\n",
        "MAX_STEPS      = 40                            #@param {type:\"number\"}\n",
        "BATCH_SIZE     = 2                             #@param {type:\"number\"}\n",
        "GRAD_ACCUM     = 4                             #@param {type:\"number\"}\n",
        "LR             = 2e-4                          #@param {type:\"number\"}\n",
        "SEED           = 42                            #@param {type:\"number\"}\n",
        "MAX_LEN        = 384                           #@param {type:\"number\"}\n",
        "\n",
        "\n",
        "MASTER_SALT = os.getenv(\"MASTER_SALT\", \"\")\n",
        "\n",
        "\n",
        "# ---------- Workspace ----------\n",
        "WORK      = Path(\"/content/phase2_work\")\n",
        "RAW_DIR   = WORK/\"raw\"\n",
        "CANON_DIR = WORK/\"canon\"\n",
        "TOK_DIR   = WORK/\"tokenized\"\n",
        "ARTIF_DIR = WORK/\"artifacts\"\n",
        "\n",
        "for d in [WORK, RAW_DIR, CANON_DIR, TOK_DIR, ARTIF_DIR]:\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"OK: config and folders ready:\", WORK)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znx6A_rQCX93"
      },
      "source": [
        "**Imports & helpers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dq-QthzUBjI-"
      },
      "outputs": [],
      "source": [
        "import os, io, json, time, gzip, shutil, zipfile, random, math, typing, tarfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "from web3 import Web3\n",
        "from eth_account import Account\n",
        "from eth_account.signers.local import LocalAccount\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForCausalLM,\n",
        "    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "def keccak_hex(b: bytes) -> str:\n",
        "    return \"0x\" + Web3.keccak(b).hex()[2:]\n",
        "\n",
        "def canonical_json(obj) -> bytes:\n",
        "    # Sorted keys + tight separators for deterministic hashing\n",
        "    return json.dumps(obj, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False).encode(\"utf-8\")\n",
        "\n",
        "def pin_json(payload: dict) -> str:\n",
        "    # Priority: IPFS proxy -> Pinata -> web3.storage -> dummy\n",
        "    if IPFS_PROXY:\n",
        "        r = requests.post(f\"{IPFS_PROXY.rstrip('/')}/ipfs/pin_json\", json=payload, timeout=120)\n",
        "        r.raise_for_status(); return r.json()[\"cid\"]\n",
        "    if PINATA_JWT:\n",
        "        r = requests.post(\"https://api.pinata.cloud/pinning/pinJSONToIPFS\",\n",
        "                          headers={\"Authorization\": f\"Bearer {PINATA_JWT}\"}, json=payload, timeout=120)\n",
        "        r.raise_for_status(); return r.json()[\"IpfsHash\"]\n",
        "    if W3S_TOKEN:\n",
        "        data = io.BytesIO(json.dumps(payload).encode(\"utf-8\"))\n",
        "        r = requests.post(\"https://api.web3.storage/upload\",\n",
        "                          headers={\"Authorization\": f\"Bearer {W3S_TOKEN}\"}, data=data.getvalue(), timeout=120)\n",
        "        r.raise_for_status(); return r.json()[\"cid\"]\n",
        "    return \"bafkrei\" + \"0\"*44  # dummy so you can test chain flow\n",
        "\n",
        "def pin_file(path: Path) -> str:\n",
        "    content = path.read_bytes()\n",
        "    if IPFS_PROXY:\n",
        "        r = requests.post(f\"{IPFS_PROXY.rstrip('/')}/ipfs/pin_file\",\n",
        "                          files={\"file\": (path.name, content, \"application/octet-stream\")}, timeout=300)\n",
        "        r.raise_for_status(); return r.json()[\"cid\"]\n",
        "    if PINATA_JWT:\n",
        "        r = requests.post(\"https://api.pinata.cloud/pinning/pinFileToIPFS\",\n",
        "                          headers={\"Authorization\": f\"Bearer {PINATA_JWT}\"},\n",
        "                          files={\"file\": (path.name, content, \"application/octet-stream\")}, timeout=300)\n",
        "        r.raise_for_status(); return r.json()[\"IpfsHash\"]\n",
        "    if W3S_TOKEN:\n",
        "        r = requests.post(\"https://api.web3.storage/upload\",\n",
        "                          headers={\"Authorization\": f\"Bearer {W3S_TOKEN}\"}, data=content, timeout=300)\n",
        "        r.raise_for_status(); return r.json()[\"cid\"]\n",
        "    return \"bafkrei\" + \"0\"*44\n",
        "\n",
        "def to_bytes32(hexstr: str) -> bytes:\n",
        "    assert hexstr.startswith(\"0x\") and len(hexstr) == 66, f\"bad bytes32: {hexstr}\"\n",
        "    return bytes.fromhex(hexstr[2:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFe6lp03CQG8"
      },
      "source": [
        "**Web3 setup & ABIs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajoitXe_BjLk"
      },
      "outputs": [],
      "source": [
        "# Connect\n",
        "w3 = Web3(Web3.HTTPProvider(RPC_URL))\n",
        "assert w3.is_connected(), \"RPC not reachable (check RPC_URL/Infura)\"\n",
        "acct: LocalAccount = Account.from_key(PRIVATE_KEY if PRIVATE_KEY.startswith(\"0x\") else \"0x\"+PRIVATE_KEY)\n",
        "print(\"Using account:\", acct.address)\n",
        "chain_id = w3.eth.chain_id\n",
        "print(\"Chain ID:\", chain_id)\n",
        "\n",
        "# Minimal ABIs limited to functions we call\n",
        "PHASE1_ABI = json.loads(\"\"\"<add_abi>\"\"\")\n",
        "\n",
        "\n",
        "PHASE2_ABI = (json.loads(\"\"\"<add_abi>\"\"\")\n",
        "\n",
        "phase1 = w3.eth.contract(address=Web3.to_checksum_address(PHASE1_ADDR), abi=PHASE1_ABI)\n",
        "phase2 = w3.eth.contract(address=Web3.to_checksum_address(PHASE2_ADDR), abi=PHASE2_ABI)\n",
        "\n",
        "def send_tx(fn, *args, gas=None):\n",
        "    nonce = w3.eth.get_transaction_count(acct.address)\n",
        "    tx = fn(*args).build_transaction({\n",
        "        \"from\": acct.address,\n",
        "        \"nonce\": nonce,\n",
        "        \"chainId\": chain_id,\n",
        "    })\n",
        "    # EIP-1559 fees (safe-ish defaults)\n",
        "    latest = w3.eth.get_block(\"latest\")\n",
        "    base = latest.get(\"baseFeePerGas\", w3.to_wei(1, \"gwei\"))\n",
        "    max_priority = w3.to_wei(2, \"gwei\")\n",
        "    tx[\"maxPriorityFeePerGas\"] = max_priority\n",
        "    tx[\"maxFeePerGas\"] = base + max_priority * 2\n",
        "    if gas is None:\n",
        "        try:\n",
        "            tx[\"gas\"] = int(w3.eth.estimate_gas(tx) * 1.2)\n",
        "        except Exception:\n",
        "            tx[\"gas\"] = 600_000\n",
        "    else:\n",
        "        tx[\"gas\"] = gas\n",
        "    signed = acct.sign_transaction(tx)\n",
        "    txh = w3.eth.send_raw_transaction(signed.rawTransaction)\n",
        "    rcpt = w3.eth.wait_for_transaction_receipt(txh)\n",
        "    return rcpt\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NAiqpO2CN1L"
      },
      "source": [
        "**Phase-1: Verify dataset root**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3zKxKzpBjNx"
      },
      "outputs": [],
      "source": [
        "# If DATASET_VER isn't set, or is 0, pick latest\n",
        "if 'DATASET_VER' not in globals() or not int(globals().get('DATASET_VER') or 0):\n",
        "    _vers = phase1.functions.getDatasetVersions(Web3.to_bytes(hexstr=DATASET_ID_HEX)).call()\n",
        "    assert _vers, \"No versions registered yet for this datasetId\"\n",
        "    DATASET_VER = int(_vers[-1][4])\n",
        "\n",
        "\n",
        "ds_id = Web3.to_bytes(hexstr=DATASET_ID_HEX)\n",
        "versions = phase1.functions.getDatasetVersions(ds_id).call()\n",
        "assert len(versions) >= DATASET_VER, f\"Dataset version {DATASET_VER} not found on chain\"\n",
        "onchain_root = Web3.to_hex(versions[DATASET_VER-1][0])\n",
        "print(f\"On-chain datasetRoot (v{DATASET_VER}):\", onchain_root)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTihxNYVCL9j"
      },
      "source": [
        "**Download + canonicalize a small sample**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYl4xowpBjP0"
      },
      "outputs": [],
      "source": [
        "import gzip\n",
        "\n",
        "zip_path = WORK / \"mimic_demo.zip\"\n",
        "if ZIP_URL and not zip_path.exists():\n",
        "    r = requests.get(ZIP_URL, stream=True); r.raise_for_status()\n",
        "    with open(zip_path, \"wb\") as f:\n",
        "        for chunk in r.iter_content(8192): f.write(chunk)\n",
        "\n",
        "if not any(RAW_DIR.iterdir()):\n",
        "    import zipfile\n",
        "    with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
        "        zf.extractall(RAW_DIR)\n",
        "\n",
        "def read_csv_robust(path, nrows=None):\n",
        "    try:\n",
        "        return pd.read_csv(path, dtype=str, engine=\"c\", low_memory=False, nrows=nrows)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return pd.read_csv(path, dtype=str, engine=\"python\", on_bad_lines=\"skip\", nrows=nrows)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def canonicalize_csv_sample(in_path: Path, out_path: Path, nrows: int = 200):\n",
        "    df = read_csv_robust(in_path, nrows=nrows)\n",
        "    if df is None or df.empty: return False\n",
        "    cols = sorted(df.columns)\n",
        "    df = df[cols].sort_values(by=cols, na_position=\"last\").reset_index(drop=True)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8\", lineterminator=\"\\n\")\n",
        "    return True\n",
        "\n",
        "canon_count = 0\n",
        "sources = list(sorted(RAW_DIR.rglob(\"*.csv\"))) + list(sorted(RAW_DIR.rglob(\"*.csv.gz\")))\n",
        "for src in sources:\n",
        "    to_read = src\n",
        "    if src.suffix == \".gz\" and src.name.endswith(\".csv.gz\"):\n",
        "        to_read = src.with_suffix(\"\")  # strip one suffix\n",
        "        with gzip.open(src, \"rb\") as fin, open(to_read, \"wb\") as fout:\n",
        "            shutil.copyfileobj(fin, fout)\n",
        "    out = CANON_DIR / Path(to_read).name\n",
        "    if canonicalize_csv_sample(to_read, out): canon_count += 1\n",
        "\n",
        "assert canon_count > 0, \"No CSV files canonicalized\"\n",
        "print(\"Canonicalized CSVs:\", canon_count)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYGeMbOJCJuT"
      },
      "source": [
        "**Build tiny SFT dataset + tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rorZRK2YBjSI"
      },
      "outputs": [],
      "source": [
        "def build_pairs(csv_path: Path, max_rows=120):\n",
        "    df = read_csv_robust(csv_path, nrows=max_rows)\n",
        "    if df is None or df.empty: return []\n",
        "    df = df.fillna(\"\")\n",
        "    samples = []\n",
        "    for _, row in df.iterrows():\n",
        "        rd = row.to_dict()\n",
        "        prompt = (\n",
        "            f\"You are a clinical assistant. Given the following record from \"\n",
        "            f\"{csv_path.name}, provide a short summary:\\n{rd}\"\n",
        "        )\n",
        "        response = \"Summary: [Your summary here]\"\n",
        "        samples.append({\"prompt\": prompt, \"response\": response})\n",
        "    return samples\n",
        "\n",
        "pairs = []\n",
        "for csv in sorted(CANON_DIR.glob(\"*.csv\")):\n",
        "    pairs += build_pairs(csv, max_rows=120)\n",
        "assert len(pairs) > 0, \"No training pairs built\"\n",
        "len(pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SydBpSHIBjUa"
      },
      "outputs": [],
      "source": [
        "# Tokenizer FIRST (so we don't use it before it's created)\n",
        "tok = AutoTokenizer.from_pretrained(MODEL_REPO, use_fast=True)\n",
        "if tok.pad_token is None:\n",
        "    tok.pad_token = tok.eos_token\n",
        "\n",
        "def format_sample(ex):\n",
        "    return {\"text\": ex[\"prompt\"] + \"\\n\" + ex[\"response\"]}\n",
        "\n",
        "from datasets import Dataset\n",
        "ds = Dataset.from_list([format_sample(x) for x in pairs])\n",
        "\n",
        "def tok_fn(batch):\n",
        "    return tok(batch[\"text\"], truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "ds_tok = ds.map(tok_fn, batched=True, remove_columns=[\"text\"]).shuffle(seed=SEED)\n",
        "print(ds_tok)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EP4kTmFCHlg"
      },
      "source": [
        "**Model + LoRA (no bitsandbytes), train**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fk-iR_dWBjWp"
      },
      "outputs": [],
      "source": [
        "bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
        "dtype = torch.bfloat16 if bf16_ok else torch.float16\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_REPO, torch_dtype=dtype, device_map=None\n",
        ").to(\"cuda\")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "peft_cfg = LoraConfig(\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\"]\n",
        ")\n",
        "\n",
        "# IMPORTANT when using gradient checkpointing\n",
        "base_model.config.use_cache = False\n",
        "model = get_peft_model(base_model, peft_cfg)\n",
        "\n",
        "# Make inputs require grad for checkpointing\n",
        "if hasattr(model, \"enable_input_require_grads\"):\n",
        "    model.enable_input_require_grads()\n",
        "\n",
        "# Enable gradient checkpointing (new HF prefers use_reentrant=False)\n",
        "try:\n",
        "    model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
        "except TypeError:\n",
        "    # for slightly older transformers\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "# Opt-in LoRA params to train\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "import random, numpy as np, torch\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tok, mlm=False)\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=str(WORK/\"out\"),\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRAD_ACCUM,\n",
        "    learning_rate=LR,\n",
        "    num_train_epochs=1,\n",
        "    max_steps=MAX_STEPS,\n",
        "    logging_steps=10,\n",
        "    save_steps=MAX_STEPS,\n",
        "    save_total_limit=1,\n",
        "    remove_unused_columns=False,\n",
        "    bf16=bf16_ok,\n",
        "    fp16=not bf16_ok,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"none\",\n",
        "    gradient_checkpointing=True,   # let Trainer cooperate with GC\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    tokenizer=tok,  # FutureWarning is fine; still works in 4.x\n",
        "    train_dataset=ds_tok,\n",
        "    data_collator=collator\n",
        ")\n",
        "\n",
        "train_res = trainer.train()\n",
        "print(\"Train done; steps:\", train_res.global_step)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hD1X9JB3Z4S"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Extract logs from Trainer state\n",
        "logs = trainer.state.log_history\n",
        "\n",
        "steps = []\n",
        "losses = []\n",
        "\n",
        "for entry in logs:\n",
        "    if \"loss\" in entry and \"step\" in entry:\n",
        "        steps.append(entry[\"step\"])\n",
        "        losses.append(entry[\"loss\"])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(steps, losses, marker='o', label=\"Training Loss\")\n",
        "plt.xlabel(\"Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss over Steps\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "\n",
        "# Save plot to file\n",
        "plt.savefig(\"training_loss_plot.png\", dpi=300)\n",
        "\n",
        "print(\"Plot saved as training_loss_plot.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-tuSdfMCFJy"
      },
      "source": [
        "**Save adapter, bundle artifacts, hashes, pin**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z16PIccRBjbH"
      },
      "outputs": [],
      "source": [
        "import tarfile\n",
        "\n",
        "lora_dir = ARTIF_DIR/\"lora\"\n",
        "lora_dir.mkdir(exist_ok=True, parents=True)\n",
        "model.save_pretrained(lora_dir)\n",
        "tok.save_pretrained(lora_dir)\n",
        "\n",
        "weights_tar = ARTIF_DIR/\"lora_adapter.tar.gz\"\n",
        "with tarfile.open(weights_tar, \"w:gz\") as tar:\n",
        "    tar.add(lora_dir, arcname=\"lora_adapter\")\n",
        "\n",
        "training_config = {\n",
        "    \"datasetId\": DATASET_ID_HEX,\n",
        "    \"datasetVersion\": DATASET_VER,\n",
        "    \"datasetRoot\": onchain_root,\n",
        "    \"modelRepo\": MODEL_REPO,\n",
        "    \"lora\": {\"r\": peft_cfg.r, \"alpha\": peft_cfg.lora_alpha, \"dropout\": peft_cfg.lora_dropout},\n",
        "    \"train\": {\"max_steps\": MAX_STEPS, \"batch\": BATCH_SIZE, \"grad_accum\": GRAD_ACCUM, \"lr\": LR, \"seed\": SEED}\n",
        "}\n",
        "cfg_path = ARTIF_DIR/\"training_config.json\"\n",
        "cfg_path.write_text(json.dumps(training_config, indent=2))\n",
        "config_hash = keccak_hex(canonical_json(training_config))\n",
        "\n",
        "metrics = {\n",
        "    \"final_loss\": float(trainer.state.log_history[-1].get(\"loss\", 0.0)),\n",
        "    \"steps\": int(train_res.global_step),\n",
        "    \"seed\": SEED\n",
        "}\n",
        "metrics_path = ARTIF_DIR/\"metrics.json\"\n",
        "metrics_path.write_text(json.dumps(metrics, indent=2))\n",
        "metrics_hash = keccak_hex(canonical_json(metrics))\n",
        "\n",
        "weights_hash = keccak_hex(weights_tar.read_bytes())\n",
        "print(\"configHash:\", config_hash)\n",
        "print(\"metricsHash:\", metrics_hash)\n",
        "print(\"weightsHash:\", weights_hash)\n",
        "\n",
        "# Pack artifacts for pinning\n",
        "artifacts_tar = ARTIF_DIR/\"artifacts.tar.gz\"\n",
        "with tarfile.open(artifacts_tar, \"w:gz\") as tar:\n",
        "    tar.add(weights_tar, arcname=\"lora_adapter.tar.gz\")\n",
        "    tar.add(cfg_path, arcname=\"training_config.json\")\n",
        "    tar.add(metrics_path, arcname=\"metrics.json\")\n",
        "\n",
        "artifacts_cid = pin_file(artifacts_tar)\n",
        "print(\"artifacts CID:\", artifacts_cid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SeHsjGbCCr0"
      },
      "source": [
        "**On-chain: create model → start run → finalize**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pmm2osowg6sV"
      },
      "outputs": [],
      "source": [
        "# Option 2 (clean): create NEW model (auto-generated modelId) + alsoAnchor = true\n",
        "# Uses existing names: ds_id, DATASET_VER, config_hash, phase2, send_tx, acct, weights_hash, metrics_hash, artifacts_cid\n",
        "\n",
        "# 1) Prepare args\n",
        "zero32      = b\"\\x00\" * 32\n",
        "init_cfg_b  = to_bytes32(config_hash)                          # bytes32 from your training_config hash\n",
        "code_hash_b = globals().get(\"code_hash_bytes32\", zero32)       # optional\n",
        "arch_hash_b = globals().get(\"arch_hash_bytes32\", zero32)       # optional\n",
        "model_uri   = globals().get(\"model_uri_string\", f\"ipfs://{artifacts_cid}\" if \"artifacts_cid\" in globals() else \"\")\n",
        "\n",
        "# 2) Create model with auto-generated modelId and anchor = true\n",
        "rcpt = send_tx(\n",
        "    phase2.functions.createModel,\n",
        "    zero32,            # modelId=0 => contract autogenerates\n",
        "    ds_id,             # bytes32 (built earlier from DATASET_ID_HEX)\n",
        "    int(DATASET_VER),  # uint256\n",
        "    code_hash_b,       # bytes32\n",
        "    arch_hash_b,       # bytes32\n",
        "    model_uri,         # string\n",
        "    init_cfg_b,        # bytes32 (configHash)\n",
        "    True               # alsoAnchor\n",
        ")\n",
        "print(\"createModel tx:\", rcpt.transactionHash.hex())\n",
        "\n",
        "# 3) Extract the NEW modelId from ModelCreated event\n",
        "try:\n",
        "    evs = phase2.events.ModelCreated().process_receipt(rcpt)\n",
        "    assert evs, \"ModelCreated event not found in receipt\"\n",
        "    new_model_id = evs[0][\"args\"][\"modelId\"]  # bytes32\n",
        "except Exception:\n",
        "    # Fallback: decode via topic signature (no ABI mismatch warnings)\n",
        "    MODEL_CREATED_SIG = Web3.keccak(\n",
        "        text=\"ModelCreated(bytes32,address,bytes32,uint256,bytes32,bytes32,bytes32,string)\"\n",
        "    ).hex()\n",
        "    new_model_id = None\n",
        "    for lg in rcpt.logs:\n",
        "        if lg[\"address\"].lower() == phase2.address.lower() and lg[\"topics\"]:\n",
        "            if lg[\"topics\"][0].hex().lower() == MODEL_CREATED_SIG.lower():\n",
        "                new_model_id = Web3.to_bytes(hexstr=lg[\"topics\"][1].hex())\n",
        "                break\n",
        "    assert new_model_id is not None, \"Could not extract new modelId from logs\"\n",
        "\n",
        "print(\"New modelId:\", \"0x\" + new_model_id.hex())\n",
        "\n",
        "# 4) Wire the new id for later cells\n",
        "model_id_bytes = new_model_id\n",
        "print(\"model_id_bytes set.\")\n",
        "\n",
        "# 5) Start + finalize a run under THIS new model\n",
        "run_receipt = send_tx(phase2.functions.startTrainingRun, model_id_bytes, init_cfg_b)\n",
        "ev = phase2.events.TrainingStarted().process_receipt(run_receipt)\n",
        "run_id = int(ev[0][\"args\"][\"runId\"]) if ev else None\n",
        "print(\"runId:\", run_id)\n",
        "\n",
        "final_receipt = send_tx(\n",
        "    phase2.functions.finalizeTrainingRun,\n",
        "    run_id,\n",
        "    Web3.to_bytes(hexstr=weights_hash),   # NOTE: snake_case variable names from earlier cell\n",
        "    Web3.to_bytes(hexstr=metrics_hash),\n",
        "    f\"ipfs://{artifacts_cid}\" if \"artifacts_cid\" in globals() else \"\"\n",
        ")\n",
        "print(\"finalized run tx:\", final_receipt.transactionHash.hex())\n",
        "print(\"✅ New model created, anchored, run started+finalized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAF236avsRY2"
      },
      "source": [
        "**Deterministic salts [Still being tested]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANGAGW0JsRmE"
      },
      "outputs": [],
      "source": [
        "# --- Deterministic salts (robust) ---\n",
        "import os, hmac, hashlib, secrets, base64, string\n",
        "HEX = set(string.hexdigits)\n",
        "\n",
        "def _load_master_salt():\n",
        "    s = os.getenv(\"MASTER_SALT\")\n",
        "    if not s:\n",
        "        key = secrets.token_bytes(32)\n",
        "        # Print ONLY if we just generated it (so you can copy/save it securely).\n",
        "        print(\"⚠️ Save this MASTER_SALT (hex) somewhere safe:\", key.hex())\n",
        "        return key\n",
        "    if isinstance(s, bytes):\n",
        "        if len(s) != 32:\n",
        "            raise ValueError(\"MASTER_SALT bytes must be exactly 32 bytes\")\n",
        "        return s\n",
        "    # str: try hex or base64\n",
        "    t = s.strip()\n",
        "    if t.lower().startswith(\"0x\"):\n",
        "        t = t[2:]\n",
        "    if len(t) in (64,) and all(c in HEX for c in t):\n",
        "        return bytes.fromhex(t)\n",
        "    try:\n",
        "        key = base64.b64decode(t, validate=True)\n",
        "        if len(key) != 32:\n",
        "            raise ValueError\n",
        "        return key\n",
        "    except Exception:\n",
        "        raise ValueError(\"MASTER_SALT must be 32 bytes (hex '0x..' or base64)\")\n",
        "\n",
        "MASTER_SALT = _load_master_salt()\n",
        "\n",
        "def derive_salt(master: bytes, model_id_b32: bytes, idx: int, tag: bytes, nbytes: int = 16) -> bytes:\n",
        "    \"\"\"\n",
        "    salt = HMAC_SHA256(master, model_id_b32 || uint64_be(idx) || tag)[:nbytes]\n",
        "    - master: 32 bytes\n",
        "    - model_id_b32: exactly 32 bytes\n",
        "    - idx: stable per-inference index in the batch\n",
        "    - tag: e.g., b'in' or b'out' (domain separation)\n",
        "    - nbytes: 16 or 32 (default 16 = 128-bit)\n",
        "    \"\"\"\n",
        "    if not (isinstance(master, (bytes, bytearray)) and len(master) == 32):\n",
        "        raise ValueError(\"master must be 32 bytes\")\n",
        "    if not (isinstance(model_id_b32, (bytes, bytearray)) and len(model_id_b32) == 32):\n",
        "        raise ValueError(\"model_id_b32 must be 32 bytes\")\n",
        "    if not isinstance(idx, int) or idx < 0:\n",
        "        raise ValueError(\"idx must be a non-negative int\")\n",
        "    if not (isinstance(tag, (bytes, bytearray)) and 1 <= len(tag) <= 16):\n",
        "        raise ValueError(\"tag must be 1..16 bytes\")\n",
        "    if nbytes not in (16, 32):\n",
        "        raise ValueError(\"nbytes should be 16 or 32\")\n",
        "    msg = model_id_b32 + idx.to_bytes(8, \"big\") + bytes(tag)\n",
        "    return hmac.new(master, msg, hashlib.sha256).digest()[:nbytes]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F31kpxQ5B5Lw"
      },
      "source": [
        "**Inference + Merkle batch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "446Y5lQCBjf1"
      },
      "outputs": [],
      "source": [
        "# --- helpers for ΔNLL ---\n",
        "import torch, numpy as np, json\n",
        "SEED = globals().get(\"SEED\", 42)\n",
        "\n",
        "# pick the tokenizer you actually have in scope\n",
        "_tok = tokenizer if \"tokenizer\" in globals() else tok\n",
        "\n",
        "def norm_ipfs(value: str) -> str:\n",
        "    \"\"\"Return a clean ipfs://… URI no matter what you pass in (CID or ipfs://CID).\"\"\"\n",
        "    s = normalize_cid(value)\n",
        "    return f\"ipfs://{s}\" if s else \"\"\n",
        "\n",
        "def normalize_cid(value: str) -> str:\n",
        "    s = (value or \"\").strip().strip('\"').strip(\"'\")\n",
        "    return s[7:] if s.startswith(\"ipfs://\") else s\n",
        "\n",
        "\n",
        "def generate(text, max_new=128):\n",
        "    inputs = _tok(text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new, do_sample=False)\n",
        "    return _tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "def total_nll_on_target(model, tok, prompt: str, target: str) -> float:\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    with torch.no_grad():\n",
        "        enc_p = tok(prompt, return_tensors=\"pt\").to(device)\n",
        "        enc_t = tok(target, return_tensors=\"pt\", add_special_tokens=False).to(device)\n",
        "        input_ids = torch.cat([enc_p[\"input_ids\"], enc_t[\"input_ids\"]], dim=1)\n",
        "        attn_mask = torch.cat([enc_p[\"attention_mask\"], enc_t[\"attention_mask\"]], dim=1)\n",
        "        labels = input_ids.clone()\n",
        "        labels[:, :enc_p[\"input_ids\"].shape[1]] = -100  # ignore prompt\n",
        "        out = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "        tgt_tokens = (labels != -100).sum().item()\n",
        "        return float(out.loss) * max(tgt_tokens, 1)\n",
        "\n",
        "def token_drop_report(record_text: str, keys: list[str], base_out: str) -> dict:\n",
        "    base_nll = total_nll_on_target(model, _tok, record_text, base_out)\n",
        "    attrs = []\n",
        "    for k in keys:\n",
        "        masked = record_text.replace(k, \"\")\n",
        "        ablated = total_nll_on_target(model, _tok, masked, base_out)\n",
        "        attrs.append({\"feature\": k, \"delta_nll\": float(ablated - base_nll)})\n",
        "    return {\"base_output\": base_out[:400], \"attributions\": attrs}\n",
        "\n",
        "rng = np.random.default_rng(SEED + 1)\n",
        "events = []\n",
        "for i in range(3):  # a small batch\n",
        "    ex = pairs[rng.integers(0, len(pairs))]\n",
        "    prompt = ex[\"prompt\"]\n",
        "    out = generate(prompt)\n",
        "\n",
        "    # --- deterministic salts (persistable via MASTER_SALT + (model_id, idx, tag)) ---\n",
        "    input_salt  = derive_salt(MASTER_SALT, model_id_bytes, i, b\"in\")\n",
        "    output_salt = derive_salt(MASTER_SALT, model_id_bytes, i, b\"out\")\n",
        "\n",
        "    # Show the prompt and output for inspection / logging\n",
        "    print(f\"\\n=== Inference {i} ===\")\n",
        "    print(\"Prompt:\\n\", prompt)\n",
        "    print(\"Output:\\n\", out)\n",
        "\n",
        "    # Optionally, save to a local log file for future proofs\n",
        "    with open(\"inference_log.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "        json.dump({\n",
        "            \"idx\": i,\n",
        "            \"prompt\": prompt,\n",
        "            \"output\": out,\n",
        "            \"input_salt_hex\": \"0x\" + input_salt.hex(),\n",
        "            \"output_salt_hex\": \"0x\" + output_salt.hex()\n",
        "        }, f)\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "    input_hash  = keccak_hex(input_salt  + prompt.encode(\"utf-8\"))\n",
        "    output_hash = keccak_hex(output_salt + out.encode(\"utf-8\"))\n",
        "\n",
        "    # crude feature list from prompt text\n",
        "    keys = [p.split(\":\")[0].strip() for p in prompt.split(\",\") if \":\" in p][:6] or \\\n",
        "           [\"age\",\"sex\",\"admission\",\"diagnosis\",\"med\",\"lab\"]\n",
        "\n",
        "    # --- stronger XAI: token-drop ΔNLL instead of length delta ---\n",
        "    xai = token_drop_report(prompt, keys, out)\n",
        "    xai_bytes = canonical_json({\n",
        "        \"prompt_sha\": input_hash,\n",
        "        \"output_sha\": output_hash,\n",
        "        \"method\": \"token_drop_nll\",\n",
        "        \"report\": xai\n",
        "    })\n",
        "    xai_hash = keccak_hex(xai_bytes)\n",
        "    try:\n",
        "        xai_cid = pin_json(json.loads(xai_bytes.decode(\"utf-8\")))\n",
        "    except Exception:\n",
        "        xai_cid = \"bafkrei\" + \"1\"*44\n",
        "\n",
        "    events.append({\n",
        "        \"inputHash\":  input_hash,\n",
        "        \"outputHash\": output_hash,\n",
        "        \"xaiHash\":    xai_hash,\n",
        "        \"xaiCID\":     norm_ipfs(xai_cid)\n",
        "    })\n",
        "\n",
        "len(events), events[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVn6nnZQXsAI"
      },
      "outputs": [],
      "source": [
        "# --- Token-drop ΔNLL bar chart---\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "from textwrap import shorten\n",
        "\n",
        "def plot_token_drop_delta_nll(xai_obj,\n",
        "                              save_path=\"token_drop_delta_nll.png\",\n",
        "                              sort_by_value=False,\n",
        "                              max_label_len=28):\n",
        "    \"\"\"\n",
        "    xai_obj: dict or JSON string with structure:\n",
        "      {\"method\":\"token_drop_nll\", \"report\":{\"attributions\":[{\"delta_nll\":..., \"feature\": ...}, ...]}}\n",
        "    \"\"\"\n",
        "    # Accept dict or JSON string\n",
        "    if isinstance(xai_obj, str):\n",
        "        xai_obj = json.loads(xai_obj)\n",
        "\n",
        "    report = xai_obj.get(\"report\", xai_obj)\n",
        "    attrs = report.get(\"attributions\", [])\n",
        "\n",
        "    vals = [float(a.get(\"delta_nll\", 0.0)) for a in attrs]\n",
        "    raw_labels = [str(a.get(\"feature\", \"\")).strip().strip(\"'\").strip('\"') for a in attrs]\n",
        "\n",
        "    # Nice labels for the features you showed\n",
        "    def pretty_label(s: str) -> str:\n",
        "        mapping = {\n",
        "            \"provide a short summary\": \"Instruction (summary)\",\n",
        "            \"amountuom\": \"Amount UOM\",\n",
        "            \"caregiver_id\": \"Caregiver ID\",\n",
        "            \"endtime\": \"End Time\",\n",
        "            \"hadm_id\": \"Admission ID (HADM)\",\n",
        "            \"itemid\": \"Item ID\",\n",
        "        }\n",
        "        return mapping.get(s, s.replace(\"_\", \" \").title())\n",
        "\n",
        "    labels = [pretty_label(s) for s in raw_labels]\n",
        "\n",
        "    # Optional: sort by ΔNLL descending\n",
        "    if sort_by_value:\n",
        "        pairs = sorted(zip(labels, vals), key=lambda x: x[1], reverse=True)\n",
        "        if pairs:\n",
        "            labels, vals = map(list, zip(*pairs))\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6.5, 3.8))\n",
        "    ax.bar(range(len(vals)), vals)\n",
        "    ax.set_xticks(range(len(labels)))\n",
        "    ax.set_xticklabels([shorten(lbl, width=max_label_len, placeholder=\"…\") for lbl in labels],\n",
        "                       rotation=25, ha=\"right\")\n",
        "    ax.set_ylabel(\"ΔNLL (token-drop)\")\n",
        "    ax.set_title(\"Token-drop ΔNLL attributions\")\n",
        "    ax.grid(axis=\"y\", linestyle=\":\", linewidth=0.7)\n",
        "\n",
        "    fig.tight_layout()\n",
        "    fig.savefig(save_path, dpi=300, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"Saved figure → {save_path}\")\n",
        "\n",
        "# ---- Example: paste your JSON here (from your XAI bundle/report) ----\n",
        "xai_json = {\"method\":\"token_drop_nll\",\"output_sha\":\"0x055ad709fab67008ccfd75d6954f042ac52609cf8150582523cb4c9c03d421c2\",\"prompt_sha\":\"0xe82857164c571b46760ec3008cc4b3b64d1a3e1766304ca5437f4d323deebcaf\",\"report\":{\"attributions\":[{\"delta_nll\":0.8782769441604614,\"feature\":\"provide a short summary\"},{\"delta_nll\":11.175613462924957,\"feature\":\"'amountuom'\"},{\"delta_nll\":18.746244430541992,\"feature\":\"'caregiver_id'\"},{\"delta_nll\":3.121084749698639,\"feature\":\"'endtime'\"},{\"delta_nll\":14.985986232757568,\"feature\":\"'hadm_id'\"},{\"delta_nll\":7.941444456577301,\"feature\":\"'itemid'\"}],\"base_output\":\"You are a clinical assistant. Given the following record from ingredientevents.csv, provide a short summary:\\n{'amount': '158.16667138040066', 'amountuom': 'ml', 'caregiver_id': '12929', 'endtime': '2132-12-16 06:23:00', 'hadm_id': '20626031', 'itemid': '220490', 'linkorderid': '9595817', 'orderid': '9595817', 'originalamount': '0', 'originalrate': '250', 'rate': '10', 'rateuom': 'mL/hour', 'startt\"}}\n",
        "\n",
        "# Make & auto-save the figure\n",
        "plot_token_drop_delta_nll(xai_json, save_path=\"token_drop_delta_nll.png\", sort_by_value=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hF3jHRzBjiY"
      },
      "outputs": [],
      "source": [
        "# Build Merkle root over keccak32(inputHash||outputHash||xaiHash), with sorted-pair parenting\n",
        "def b32(x: str) -> bytes:\n",
        "    assert x.startswith(\"0x\") and len(x)==66\n",
        "    return bytes.fromhex(x[2:])\n",
        "\n",
        "def parent_sorted(a: bytes, b: bytes) -> bytes:\n",
        "    return Web3.keccak(a+b) if a < b else Web3.keccak(b+a)\n",
        "\n",
        "leaves = [Web3.keccak(b32(e[\"inputHash\"]) + b32(e[\"outputHash\"]) + b32(e[\"xaiHash\"])) for e in events]\n",
        "\n",
        "def merkle_root(hashes: list[bytes]) -> bytes:\n",
        "    if len(hashes) == 1:\n",
        "        return hashes[0]\n",
        "    level = hashes[:]\n",
        "    while len(level) > 1:\n",
        "        if len(level) % 2 == 1: level.append(level[-1])\n",
        "        nxt = []\n",
        "        for i in range(0,len(level),2):\n",
        "            a,b = level[i], level[i+1]\n",
        "            nxt.append(parent_sorted(a,b))\n",
        "        level = nxt\n",
        "    return level[0]\n",
        "\n",
        "root_bytes = merkle_root(leaves)\n",
        "batchRoot = \"0x\"+root_bytes.hex()\n",
        "\n",
        "manifest = {\n",
        "  \"modelId\": \"0x\" + model_id_bytes.hex(),\n",
        "  \"count\": len(events),\n",
        "  \"leaf_rule\": \"keccak32(inputHash||outputHash||xaiHash)\",\n",
        "  \"pair_rule\": \"sorted-pair keccak\",\n",
        "  \"events\": events,\n",
        "  \"created_at\": int(time.time())\n",
        "}\n",
        "manifest[\"saltScheme\"] = \"hmac_sha256(master, modelId||uint64(idx)||tag)[:16]\"\n",
        "\n",
        "batch_cid = pin_json(manifest)\n",
        "print(\"batchRoot:\", batchRoot)\n",
        "print(\"batch CID:\", batch_cid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbSm2U1AB61-"
      },
      "source": [
        "**Commit inference batch on-chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DwZJMSLmBzet"
      },
      "outputs": [],
      "source": [
        "# Safer conversion helpers\n",
        "def as_bytes32(x):\n",
        "    # Accept either bytes32 already, or \"0x...\" hex string\n",
        "    if isinstance(x, (bytes, bytearray)):\n",
        "        assert len(x) == 32, f\"expected 32 bytes, got {len(x)}\"\n",
        "        return bytes(x)\n",
        "    s = str(x).strip()\n",
        "    if s.startswith(\"0x\") or s.startswith(\"0X\"):\n",
        "        s = s[2:]\n",
        "    assert len(s) == 64, f\"expected 32-byte hex (64 nibbles), got {len(s)} nibbles\"\n",
        "    return bytes.fromhex(s)\n",
        "\n",
        "# You computed these earlier:\n",
        "# - model_id_bytes (bytes)\n",
        "# - root_bytes (bytes)  ← direct from merkle_root()\n",
        "# - batchRoot (string \"0x...\")  ← derived from root_bytes\n",
        "# - batch_cid (string)\n",
        "\n",
        "# Use the BYTES value directly (avoid hex parsing issues)\n",
        "batch_root_bytes = as_bytes32(root_bytes)\n",
        "\n",
        "rcpt_batch = send_tx(\n",
        "    phase2.functions.commitInferenceBatch,\n",
        "    model_id_bytes,\n",
        "    batch_root_bytes,\n",
        "    len(manifest[\"events\"]),\n",
        "    norm_ipfs(batch_cid)\n",
        ")\n",
        "\n",
        "print(\"commitInferenceBatch tx:\", rcpt_batch.transactionHash.hex())\n",
        "print(\"DONE ✅\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBRbZp6VS-JT"
      },
      "source": [
        "**Reveal script**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgwMdlXABzhA"
      },
      "outputs": [],
      "source": [
        "# --- AUTO: use existing PHASE2_ABI (or existing `phase2`) to fetch latest batch CID+root ---\n",
        "import json\n",
        "from web3 import Web3\n",
        "\n",
        "try:\n",
        "    RPC_URL\n",
        "except NameError:\n",
        "    RPC_URL = <YOUR_RPC_URL>\n",
        "\n",
        "try:\n",
        "    PHASE2_ADDR\n",
        "except NameError:\n",
        "    raise RuntimeError(\"Set PHASE2_ADDR before running this cell\")\n",
        "\n",
        "w3 = Web3(Web3.HTTPProvider(RPC_URL))\n",
        "assert w3.is_connected(), \"RPC not reachable\"\n",
        "\n",
        "# Reuse contract instance if present; otherwise build it with PHASE2_ABI you loaded earlier\n",
        "try:\n",
        "    phase2  # already defined elsewhere?\n",
        "    # sanity: if address differs, rebuild\n",
        "    if phase2.address.lower() != Web3.to_checksum_address(PHASE2_ADDR).lower():\n",
        "        phase2 = w3.eth.contract(address=Web3.to_checksum_address(PHASE2_ADDR), abi=PHASE2_ABI)\n",
        "except NameError:\n",
        "    phase2 = w3.eth.contract(address=Web3.to_checksum_address(PHASE2_ADDR), abi=PHASE2_ABI)\n",
        "\n",
        "def _normalize_cid(value: str) -> str:\n",
        "    s = (value or \"\").strip().strip('\"').strip(\"'\")\n",
        "    return s[7:] if s.startswith(\"ipfs://\") else s\n",
        "\n",
        "def get_latest_batch_for_model(_phase2, model_id_b: bytes, start_block=0):\n",
        "    logs = _phase2.events.InferenceBatchCommitted().get_logs(\n",
        "        fromBlock=start_block, toBlock=\"latest\", argument_filters={\"modelId\": model_id_b}\n",
        "    )\n",
        "    if not logs:\n",
        "        raise RuntimeError(\"No InferenceBatchCommitted events found for this modelId\")\n",
        "    a = logs[-1][\"args\"]\n",
        "    return _normalize_cid(a[\"batchCID\"]), \"0x\" + a[\"batchRoot\"].hex()\n",
        "\n",
        "# `model_id_bytes` must already be set earlier in your flow\n",
        "DEPLOY_BLOCK = 8984586  # optionally set to your Phase-2 deploy block\n",
        "BATCH_MANIFEST_CID, BATCH_ROOT_ONCHAIN = get_latest_batch_for_model(phase2, model_id_bytes, start_block=DEPLOY_BLOCK)\n",
        "print(\"Auto-selected BATCH_MANIFEST_CID:\", BATCH_MANIFEST_CID)\n",
        "print(\"Batch root (on-chain):\", BATCH_ROOT_ONCHAIN)\n",
        "\n",
        "# Gateways (keep as-is / or your existing list)\n",
        "IPFS_GATEWAYS = [\n",
        "    \"https://gateway.pinata.cloud/ipfs/\",\n",
        "    \"https://ipfs.io/ipfs/\",\n",
        "]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df8sPxeW_z0x"
      },
      "source": [
        "**Added because manually filling salt hex from the inference_log.jsonl didnt' work for some reason**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txaruadgefQm"
      },
      "outputs": [],
      "source": [
        "# Fill prompt_text / output_text / salts from your saved JSONL\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "JSONL_PATHS = [\n",
        "    \"/mnt/data/inference_log.jsonl\",  # attached path\n",
        "    \"inference_log.jsonl\",            # local working dir fallback\n",
        "]\n",
        "JSONL_PATH = next((p for p in JSONL_PATHS if Path(p).exists()), None)\n",
        "assert JSONL_PATH, \"inference_log.jsonl not found (checked /mnt/data and CWD).\"\n",
        "\n",
        "# Choose which row to use: \"latest\" or an integer index\n",
        "TARGET = \"latest\"  # or e.g., 0\n",
        "\n",
        "rows = []\n",
        "with open(JSONL_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        try:\n",
        "            rows.append(json.loads(line))\n",
        "        except Exception:\n",
        "            pass\n",
        "assert rows, f\"No rows in {JSONL_PATH}\"\n",
        "\n",
        "row = rows[-1] if TARGET == \"latest\" else rows[int(TARGET)]\n",
        "\n",
        "prompt_text = row[\"prompt\"]\n",
        "output_text = row[\"output\"]\n",
        "\n",
        "# Handle either hex strings or raw bytes saved as repr\n",
        "def _norm_hex(v):\n",
        "    if isinstance(v, (bytes, bytearray)):\n",
        "        return \"0x\" + bytes(v).hex()\n",
        "    s = str(v).strip().strip('\"').strip(\"'\")\n",
        "    while s.lower().startswith(\"0x\"):\n",
        "        s = s[2:]\n",
        "    return \"0x\" + s\n",
        "\n",
        "input_salt_hex  = _norm_hex(row.get(\"input_salt_hex\", row.get(\"input_salt\", \"\")))\n",
        "output_salt_hex = _norm_hex(row.get(\"output_salt_hex\", row.get(\"output_salt\", \"\")))\n",
        "\n",
        "print(\"Loaded from:\", JSONL_PATH, \"| idx:\", row.get(\"idx\"))\n",
        "print(\"input_salt_hex :\", input_salt_hex)\n",
        "print(\"output_salt_hex:\", output_salt_hex)\n",
        "print(\"prompt_text[:90]:\", prompt_text[:90].replace(\"\\n\",\" \"))\n",
        "print(\"output_text[:90]:\", output_text[:90].replace(\"\\n\",\" \"))\n",
        "\n",
        "# (Optional) quick match against an already-loaded manifest `events`\n",
        "try:\n",
        "    from web3 import Web3\n",
        "    def keccak_hex(b: bytes) -> str: return \"0x\" + Web3.keccak(b).hex()[2:]\n",
        "    ih = keccak_hex(bytes.fromhex(input_salt_hex[2:])  + prompt_text.encode(\"utf-8\"))\n",
        "    oh = keccak_hex(bytes.fromhex(output_salt_hex[2:]) + output_text.encode(\"utf-8\"))\n",
        "    if \"events\" in globals():\n",
        "        match_i = next((i for i,e in enumerate(events)\n",
        "                        if e[\"inputHash\"].lower()==ih.lower()\n",
        "                        and e[\"outputHash\"].lower()==oh.lower()), None)\n",
        "        print(\"Matched index in manifest:\", match_i)\n",
        "except Exception as e:\n",
        "    print(\"Manifest match check skipped:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtIbr0rcBzjl"
      },
      "outputs": [],
      "source": [
        "# ----- Robust reveal OR non-reveal membership check -----\n",
        "import json, requests, base64, ast, string\n",
        "from web3 import Web3\n",
        "\n",
        "HEXCHARS = set(string.hexdigits)\n",
        "\n",
        "def b32(x: str|bytes) -> bytes:\n",
        "    if isinstance(x, (bytes, bytearray)):\n",
        "        assert len(x) == 32, f\"expected 32 bytes, got {len(x)}\"\n",
        "        return bytes(x)\n",
        "    s = str(x).strip().strip('\"').strip(\"'\")\n",
        "    while s.lower().startswith(\"0x\"):  # strip ANY number of 0x prefixes\n",
        "        s = s[2:]\n",
        "    if len(s) != 64:\n",
        "        raise ValueError(f\"expected 64 hex chars (32 bytes), got {len(s)}\")\n",
        "    return bytes.fromhex(s)\n",
        "\n",
        "def keccak_hex(b: bytes) -> str:\n",
        "    return \"0x\" + Web3.keccak(b).hex()[2:]\n",
        "\n",
        "def parent_sorted(a: bytes, b: bytes) -> bytes:\n",
        "    return Web3.keccak(a+b) if a < b else Web3.keccak(b+a)\n",
        "\n",
        "def merkle_root(hashes: list[bytes]) -> bytes:\n",
        "    if not hashes: raise ValueError(\"no leaves\")\n",
        "    level = hashes[:]\n",
        "    while len(level) > 1:\n",
        "        if len(level) % 2 == 1: level.append(level[-1])\n",
        "        nxt = []\n",
        "        for i in range(0, len(level), 2):\n",
        "            nxt.append(parent_sorted(level[i], level[i+1]))\n",
        "        level = nxt\n",
        "    return level[0]\n",
        "\n",
        "def merkle_proof(hashes: list[bytes], idx: int) -> list[bytes]:\n",
        "    if not (0 <= idx < len(hashes)): raise IndexError(\"idx out of range\")\n",
        "    proof, level_idx, level = [], idx, hashes[:]\n",
        "    while len(level) > 1:\n",
        "        if len(level) % 2 == 1: level.append(level[-1])\n",
        "        nxt = []\n",
        "        for i in range(0, len(level), 2):\n",
        "            L, R = level[i], level[i+1]\n",
        "            if i == level_idx or i+1 == level_idx:\n",
        "                proof.append(R if i == level_idx else L)\n",
        "                level_idx = len(nxt)\n",
        "            nxt.append(parent_sorted(L, R))\n",
        "        level = nxt\n",
        "    return proof\n",
        "\n",
        "def normalize_cid(value: str) -> str:\n",
        "    s = (value or \"\").strip().strip('\"').strip(\"'\")\n",
        "    if s.startswith(\"ipfs://\"): s = s[7:]\n",
        "    if s.startswith(\"ipfs/\"):   s = s[5:]\n",
        "    return s\n",
        "\n",
        "def norm_ipfs(value: str) -> str:\n",
        "    \"\"\"Return a clean ipfs://… URI no matter what you pass in (CID or ipfs://CID).\"\"\"\n",
        "    s = normalize_cid(value)\n",
        "    return f\"ipfs://{s}\" if s else \"\"\n",
        "\n",
        "def fetch_manifest_json(cid: str, gateways=None) -> dict:\n",
        "    gateways = gateways or [\n",
        "        \"https://gateway.pinata.cloud/ipfs/\",\n",
        "        \"https://ipfs.io/ipfs/\",\n",
        "        \"https://dweb.link/ipfs/\",\n",
        "    ]\n",
        "    last_err = None\n",
        "    for base in gateways:\n",
        "        url = base.rstrip(\"/\") + \"/\" + cid\n",
        "        try:\n",
        "            r = requests.get(url, timeout=60, headers={\"Accept\": \"application/json\"})\n",
        "            r.raise_for_status()\n",
        "            return r.json()\n",
        "        except Exception as e:\n",
        "            last_err = e; continue\n",
        "    raise RuntimeError(f\"Could not fetch manifest {cid}: {last_err}\")\n",
        "\n",
        "def get_latest_batch_for_model(phase2_contract, model_id_b, rcpt_batch_obj=None, start_block=0):\n",
        "    if rcpt_batch_obj is not None:\n",
        "        try:\n",
        "            ev = phase2_contract.events.InferenceBatchCommitted().process_receipt(rcpt_batch_obj)\n",
        "            if ev:\n",
        "                a = ev[0][\"args\"]\n",
        "                return normalize_cid(a[\"batchCID\"]), \"0x\"+a[\"batchRoot\"].hex()\n",
        "        except Exception as e:\n",
        "            print(\"Receipt parse warn:\", e)\n",
        "    logs = phase2_contract.events.InferenceBatchCommitted().get_logs(\n",
        "        fromBlock=start_block, toBlock=\"latest\", argument_filters={\"modelId\": model_id_b}\n",
        "    )\n",
        "    if not logs: raise RuntimeError(\"No InferenceBatchCommitted events for this modelId\")\n",
        "    a = logs[-1][\"args\"]\n",
        "    return normalize_cid(a[\"batchCID\"]), \"0x\"+a[\"batchRoot\"].hex()\n",
        "\n",
        "# If phase2 isn't in scope (fallback to minimal wiring from earlier blocks)\n",
        "try:\n",
        "    phase2  # already in scope?\n",
        "except NameError:\n",
        "    phase2 = w3.eth.contract(address=Web3.to_checksum_address(PHASE2_ADDR), abi=PHASE2_ABI)\n",
        "\n",
        "# === Resolve CID & on-chain root (handles manual override or auto-pick) ===\n",
        "cid_override = normalize_cid(globals().get(\"BATCH_MANIFEST_CID\", \"\"))\n",
        "if cid_override:\n",
        "    # We still fetch the on-chain root for cross-check\n",
        "    _, batch_root_onchain = get_latest_batch_for_model(phase2, model_id_bytes, globals().get(\"rcpt_batch\"), 0)\n",
        "    cid = cid_override\n",
        "else:\n",
        "    DEPLOY_BLOCK = 0  # set to Phase-2 deploy block if you want faster scans\n",
        "    cid, batch_root_onchain = get_latest_batch_for_model(\n",
        "        phase2, model_id_bytes, globals().get(\"rcpt_batch\"), DEPLOY_BLOCK\n",
        "    )\n",
        "print(\"CID:\", cid)\n",
        "print(\"On-chain batchRoot:\", batch_root_onchain)\n",
        "\n",
        "# Fetch manifest → compute root → compare to on-chain\n",
        "manifest = fetch_manifest_json(cid)\n",
        "events = manifest[\"events\"]\n",
        "assert len(events) == manifest[\"count\"] == len(events), \"manifest count mismatch\"\n",
        "\n",
        "leaves = [Web3.keccak(b32(e[\"inputHash\"]) + b32(e[\"outputHash\"]) + b32(e[\"xaiHash\"])) for e in events]\n",
        "root_bytes = merkle_root(leaves)\n",
        "onchain_root_bytes = b32(batch_root_onchain)\n",
        "\n",
        "assert root_bytes == onchain_root_bytes, (\n",
        "    f\"Computed root != on-chain root\\n\"\n",
        "    f\"computed: 0x{root_bytes.hex()}\\n\"\n",
        "    f\"onchain : 0x{onchain_root_bytes.hex()}\"\n",
        ")\n",
        "print(\"Batch root matches on-chain:\", \"0x\" + root_bytes.hex())\n",
        "\n",
        "# Decide mode based on salts\n",
        "def looks_like_placeholder(s):\n",
        "    return isinstance(s, str) and s.strip() in (\"\", \"0x...\", \"0x\", \"...\")\n",
        "\n",
        "REVEAL_MODE = not (looks_like_placeholder(globals().get(\"input_salt_hex\",\"\")) or\n",
        "                   looks_like_placeholder(globals().get(\"output_salt_hex\",\"\")))\n",
        "print(\"Mode:\", \"FULL REVEAL\" if REVEAL_MODE else \"NON-REVEAL (membership only)\")\n",
        "\n",
        "if REVEAL_MODE:\n",
        "    def salt_from_any(x) -> bytes:\n",
        "        if isinstance(x, (bytes, bytearray)): return bytes(x)\n",
        "        s = str(x).strip().strip('\"').strip(\"'\")\n",
        "        if (s.startswith(\"b'\") and s.endswith(\"'\")) or (s.startswith('b\"') and s.endswith('\"')):\n",
        "            try: return ast.literal_eval(s)\n",
        "            except Exception: pass\n",
        "        h = s[2:] if s.lower().startswith(\"0x\") else s\n",
        "        if len(h) >= 2 and len(h) % 2 == 0 and all(c in HEXCHARS for c in h):\n",
        "            try: return bytes.fromhex(h)\n",
        "            except Exception: pass\n",
        "        try: return base64.b64decode(s, validate=True)\n",
        "        except Exception: pass\n",
        "        raise ValueError(f\"Unrecognized salt format: {x!r}\")\n",
        "\n",
        "    inp_salt = salt_from_any(input_salt_hex)\n",
        "    out_salt = salt_from_any(output_salt_hex)\n",
        "    print(f\"Salt lengths → input: {len(inp_salt)} bytes, output: {len(out_salt)} bytes\")\n",
        "\n",
        "    input_hash  = keccak_hex(inp_salt + prompt_text.encode(\"utf-8\"))\n",
        "    output_hash = keccak_hex(out_salt + output_text.encode(\"utf-8\"))\n",
        "    print(\"Recomputed inputHash :\", input_hash)\n",
        "    print(\"Recomputed outputHash:\", output_hash)\n",
        "\n",
        "    match_i = next((i for i,e in enumerate(events)\n",
        "                    if e[\"inputHash\"].lower()==input_hash.lower()\n",
        "                    and e[\"outputHash\"].lower()==output_hash.lower()), None)\n",
        "    assert match_i is not None, \"No matching event found in manifest (check salts/prompt/output)\"\n",
        "    xai_hash = events[match_i][\"xaiHash\"]\n",
        "    print(\"Matched index:\", match_i, \"xaiHash:\", xai_hash)\n",
        "else:\n",
        "    TARGET_INDEX = 0  # change if you want a different row\n",
        "    match_i = TARGET_INDEX\n",
        "    assert 0 <= match_i < len(events), \"TARGET_INDEX out of range\"\n",
        "    xai_hash = events[match_i][\"xaiHash\"]\n",
        "    input_hash = events[match_i][\"inputHash\"]\n",
        "    output_hash = events[match_i][\"outputHash\"]\n",
        "    print(f\"Picked event #{match_i} from manifest (non-reveal).\")\n",
        "\n",
        "# Build proof & verify\n",
        "proof_bytes = merkle_proof(leaves, match_i)\n",
        "print(\"Proof length:\", len(proof_bytes))\n",
        "\n",
        "batch_root_hex = \"0x\" + root_bytes.hex()  # for any later JSON/UI use\n",
        "\n",
        "ih = b32(input_hash); oh = b32(output_hash); xh = b32(xai_hash)\n",
        "ok = phase2.functions.verifyBatchMembership(ih, oh, xh, proof_bytes, root_bytes).call()\n",
        "print(\"On-chain verification result:\", ok)\n",
        "assert ok, \"verifyBatchMembership returned false (check inputs)\"\n",
        "print(\"✅ Membership proof verified\", \"(full reveal)\" if REVEAL_MODE else \"(non-reveal)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tEmpUopDP2s"
      },
      "source": [
        "**Merkle Batch Membership on-chain Verification Helpers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keArXoYUURpK"
      },
      "outputs": [],
      "source": [
        "def verify_by_index(idx: int = 0):\n",
        "    assert 0 <= idx < len(events), \"idx out of range\"\n",
        "    ih = b32(events[idx][\"inputHash\"])\n",
        "    oh = b32(events[idx][\"outputHash\"])\n",
        "    xh = b32(events[idx][\"xaiHash\"])\n",
        "    proof = merkle_proof(leaves, idx)\n",
        "    ok = phase2.functions.verifyBatchMembership(ih, oh, xh, proof, root_bytes).call()\n",
        "    print(f\"Index {idx} →\", ok)\n",
        "    return ok\n",
        "\n",
        "# Example:\n",
        "verify_by_index(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRUoGYYMbiGC"
      },
      "outputs": [],
      "source": [
        "def verify_all():\n",
        "    ok_count = 0\n",
        "    for i in range(len(events)):\n",
        "        ih = b32(events[i][\"inputHash\"]); oh = b32(events[i][\"outputHash\"]); xh = b32(events[i][\"xaiHash\"])\n",
        "        proof = merkle_proof(leaves, i)\n",
        "        ok = phase2.functions.verifyBatchMembership(ih, oh, xh, proof, root_bytes).call()\n",
        "        if ok: ok_count += 1\n",
        "        else: print(\"Mismatch at index\", i)\n",
        "    print(f\"{ok_count}/{len(events)} verified\")\n",
        "\n",
        "# Example:\n",
        "verify_all()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDdPqHeyaG-a"
      },
      "source": [
        "# Phase3: Generated Output Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74V9P6GPaMTS"
      },
      "outputs": [],
      "source": [
        "# === PHASE 3 — attach OutputAuthentication===\n",
        "import json, os\n",
        "from web3 import Web3\n",
        "\n",
        "assert 'w3' in globals() and 'acct' in globals() and 'send_tx' in globals(), \"Run Phase-2 cells first.\"\n",
        "\n",
        "# 1) Put your Remix-deployed address here (or via env PHASE3_ADDR)\n",
        "PHASE3_ADDR = os.getenv(\"PHASE3_ADDR\", \"<add_address>\")\n",
        "\n",
        "# 2) Paste the ABI JSON from Remix between the triple quotes:\n",
        "PHASE3_ABI = json.loads(r\"\"\"<add_abi>\"\"\")\n",
        "\n",
        "phase3 = w3.eth.contract(address=Web3.to_checksum_address(PHASE3_ADDR), abi=PHASE3_ABI)\n",
        "print(\"Phase-3 attached at:\", phase3.address)\n",
        "\n",
        "# Optional sanity: confirm you're the Phase-2 model owner (must match msg.sender for storeContentHash)\n",
        "try:\n",
        "    owner = phase2.functions.getModel(model_id_bytes).call()[0]  # owner is first field in your Phase-2 ABI\n",
        "    print(\"Phase-2 model owner:\", owner)\n",
        "    print(\"Your tx sender    :\", acct.address)\n",
        "except Exception as e:\n",
        "    print(\"Owner check skipped:\", e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4bXCyDQdS5g"
      },
      "source": [
        "**Generate with Phase-2 model ➜ hash raw output ➜ store in Phase-3**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJRhGRlwVNq-"
      },
      "source": [
        "*Ensuring that the correct phase2 ABI is loaded*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ya1gGyRLVQOv"
      },
      "outputs": [],
      "source": [
        "# --- Rebind phase2 with the FULL ABI\n",
        "PHASE2_ADDR = Web3.to_checksum_address(\"add_address\")\n",
        "\n",
        "PHASE2_ABI_FULL = json.loads(r\"\"\"<add_abi>\"\"\")\n",
        "\n",
        "phase2 = w3.eth.contract(address=PHASE2_ADDR, abi=PHASE2_ABI_FULL)\n",
        "\n",
        "# sanity\n",
        "ev_names = {e['name'] for e in phase2.abi if e.get('type') == 'event'}\n",
        "print(\"Has InferenceBatchCommitted?\", \"InferenceBatchCommitted\" in ev_names)\n",
        "print(\"Phase-2 address:\", phase2.address)\n",
        "\n",
        "# --- robust latest-batch helper (uses event if present; falls back to raw log scan) ---\n",
        "from eth_abi import decode as abi_decode\n",
        "\n",
        "def latest_phase2_batch(model_id_b, from_block=0, to_block=\"latest\"):\n",
        "    # A) try via ABI event\n",
        "    try:\n",
        "        ev = phase2.events.InferenceBatchCommitted()\n",
        "        logs = ev.get_logs(fromBlock=from_block, toBlock=to_block, argument_filters={\"modelId\": model_id_b})\n",
        "        if logs:\n",
        "            a = logs[-1][\"args\"]\n",
        "            cid = a.get(\"batchCID\", None)\n",
        "            root_b32 = a.get(\"batchRoot\", None)\n",
        "            root_hex = \"0x\" + root_b32.hex() if isinstance(root_b32, (bytes, bytearray)) else (root_b32 if isinstance(root_b32, str) else None)\n",
        "            return cid, root_hex\n",
        "    except Exception as e:\n",
        "        # print(\"Event path failed:\", e)\n",
        "        pass\n",
        "\n",
        "    # B) raw log scan by topic (works even if ABI is wrong)\n",
        "    topic0 = Web3.keccak(text=\"InferenceBatchCommitted(bytes32,bytes32,uint256,string)\")\n",
        "    flt = {\n",
        "        \"fromBlock\": from_block,\n",
        "        \"toBlock\": to_block,\n",
        "        \"address\": phase2.address,\n",
        "        \"topics\": [topic0, model_id_b]  # topics[1] is indexed modelId\n",
        "    }\n",
        "    logs = w3.eth.get_logs(flt)\n",
        "    if logs:\n",
        "        lg = logs[-1]\n",
        "        # topics[2] is indexed batchRoot\n",
        "        batch_root_hex = \"0x\" + lg[\"topics\"][2].hex()[2:].rjust(64, \"0\")\n",
        "        # data encodes (uint256 count, string batchCID)\n",
        "        data_bytes = bytes.fromhex(lg[\"data\"][2:])\n",
        "        try:\n",
        "            _count, batch_cid = abi_decode([\"uint256\",\"string\"], data_bytes)\n",
        "        except Exception:\n",
        "            batch_cid = None\n",
        "        return batch_cid, batch_root_hex\n",
        "\n",
        "    return None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AK8BArgYUOK"
      },
      "outputs": [],
      "source": [
        "print(\"Phase-3 phase2():\", phase3.functions.debugOwnerAndSender(model_id_bytes).call()[2])\n",
        "print(\"owner (phase2.getModel):\", phase2.functions.getModel(model_id_bytes).call()[0])\n",
        "print(\"owner (phase3.ownerOf): \", phase3.functions.ownerOf(model_id_bytes).call())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2gF41gmaMVj"
      },
      "outputs": [],
      "source": [
        "# === PHASE 3 — generate -> hash -> store (raw content authentication) ===\n",
        "import os, time, json\n",
        "import numpy as np\n",
        "from web3 import Web3\n",
        "from web3._utils.events import get_event_data\n",
        "from eth_abi import decode as abi_decode  # available with web3 install\n",
        "\n",
        "# -- tiny helpers\n",
        "def _rng():\n",
        "    return np.random.default_rng(12345)\n",
        "\n",
        "def _pick_prompt():\n",
        "    if 'pairs' in globals() and isinstance(pairs, (list, tuple)) and len(pairs) > 0:\n",
        "        i = _rng().integers(0, len(pairs))\n",
        "        cand = pairs[i]\n",
        "        # support dict or tuple forms\n",
        "        if isinstance(cand, dict) and \"prompt\" in cand:\n",
        "            return cand[\"prompt\"]\n",
        "        if isinstance(cand, (list, tuple)) and len(cand) > 0:\n",
        "            return cand[0]\n",
        "    return \"Summarize: {patient_id: 10293, admission_type: EMERGENCY, diagnosis: Pneumonia}\"\n",
        "\n",
        "_tok = tokenizer if \"tokenizer\" in globals() else tok  # whichever name you used in Phase 2\n",
        "\n",
        "def generate(text, max_new=160):\n",
        "    inputs = _tok(text, return_tensors=\"pt\").to(model.device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(**inputs, max_new_tokens=max_new, do_sample=False)\n",
        "    return _tok.decode(out[0], skip_special_tokens=True)\n",
        "\n",
        "prompt_text = _pick_prompt()\n",
        "output_text = generate(prompt_text)\n",
        "print(\"OUTPUT preview:\", output_text[:220], \"...\")\n",
        "\n",
        "# Content hash (bytes32)\n",
        "content_hash = Web3.keccak(text=output_text)\n",
        "print(\"contentHash:\", \"0x\" + content_hash.hex())\n",
        "\n",
        "# ---- Find latest Phase-2 batch (CID + root) robustly ----\n",
        "def _event_exists(contract, name: str) -> bool:\n",
        "    try:\n",
        "        # quick check against ABI JSON\n",
        "        for e in getattr(contract, \"abi\", []):\n",
        "            if e.get(\"type\") == \"event\" and e.get(\"name\") == name:\n",
        "                return True\n",
        "        return False\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _latest_batch_from_event(phase2_contract, model_id_b, from_block=0, to_block=\"latest\"):\n",
        "    \"\"\"\n",
        "    Requires the ABI to include:\n",
        "      event InferenceBatchCommitted(bytes32 indexed modelId, bytes32 indexed batchRoot, uint256 count, string batchCID);\n",
        "    Returns (cid_str, root_hex or None) or (None, None) if not found.\n",
        "    \"\"\"\n",
        "    if not _event_exists(phase2_contract, \"InferenceBatchCommitted\"):\n",
        "        raise RuntimeError(\"Phase-2 ABI lacks InferenceBatchCommitted\")\n",
        "\n",
        "    ev = phase2_contract.events.InferenceBatchCommitted()\n",
        "    logs = ev.get_logs(fromBlock=from_block, toBlock=to_block, argument_filters={\"modelId\": model_id_b})\n",
        "    if not logs:\n",
        "        return None, None\n",
        "\n",
        "    last = logs[-1][\"args\"]\n",
        "    # args likely contain indexed modelId, batchRoot + non-indexed count, batchCID\n",
        "    # web3 already decodes indexed params into args for us\n",
        "    cid = last.get(\"batchCID\", None)\n",
        "    root_b32 = last.get(\"batchRoot\", None)\n",
        "    root_hex = \"0x\" + root_b32.hex() if isinstance(root_b32, (bytes, bytearray)) else (root_b32 if isinstance(root_b32, str) else None)\n",
        "    return cid, root_hex\n",
        "\n",
        "def _latest_batch_from_txhash(phase2_contract, commit_tx_hash: str):\n",
        "    \"\"\"\n",
        "    Fallback: parse a known commit tx receipt without having the event in the ABI.\n",
        "    You must provide INFER_COMMIT_TX (env or variable) that corresponds to a commitInferenceBatch tx.\n",
        "    \"\"\"\n",
        "    if not commit_tx_hash:\n",
        "        return None, None\n",
        "    r = w3.eth.get_transaction_receipt(commit_tx_hash)\n",
        "    # topic0 = keccak(\"InferenceBatchCommitted(bytes32,bytes32,uint256,string)\")\n",
        "    topic0 = Web3.keccak(text=\"InferenceBatchCommitted(bytes32,bytes32,uint256,string)\")\n",
        "    addr = phase2_contract.address.lower()\n",
        "\n",
        "    for lg in r[\"logs\"]:\n",
        "        if lg[\"address\"].lower() != addr:\n",
        "            continue\n",
        "        if len(lg[\"topics\"]) < 3 or lg[\"topics\"][0] != topic0:\n",
        "            continue\n",
        "        # topics[1] = modelId (indexed), topics[2] = batchRoot (indexed)\n",
        "        batch_root_hex = \"0x\" + lg[\"topics\"][2].hex()[2:].rjust(64, \"0\")\n",
        "        # data encodes (uint256 count, string batchCID)\n",
        "        # decode ABI-encoded bytes; strip the 0x prefix\n",
        "        data_bytes = bytes.fromhex(lg[\"data\"][2:])\n",
        "        try:\n",
        "            count, batch_cid = abi_decode([\"uint256\", \"string\"], data_bytes)\n",
        "            return batch_cid, batch_root_hex\n",
        "        except Exception:\n",
        "            # If decode fails, still return the root; CID unknown\n",
        "            return None, batch_root_hex\n",
        "    return None, None\n",
        "\n",
        "def latest_phase2_batch(model_id_b):\n",
        "    # Path A: try event via ABI first\n",
        "    try:\n",
        "        return _latest_batch_from_event(phase2, model_id_b, from_block=0, to_block=\"latest\")\n",
        "    except Exception as e:\n",
        "        print(\"Batch via ABI event not available:\", e)\n",
        "\n",
        "    # Path B: fallback via known tx hash (optional)\n",
        "    commit_tx = os.getenv(\"INFER_COMMIT_TX\") or globals().get(\"INFER_COMMIT_TX\")\n",
        "    cid, root_hex = _latest_batch_from_txhash(phase2, commit_tx) if commit_tx else (None, None)\n",
        "    if cid or root_hex:\n",
        "        return cid, root_hex\n",
        "\n",
        "    # Path C: no info; return zeros\n",
        "    return None, None\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnRoOZJTXkC-"
      },
      "outputs": [],
      "source": [
        "cid, batch_root_hex = latest_phase2_batch(model_id_bytes)\n",
        "batch_root_b32 = bytes.fromhex(batch_root_hex[2:]) if batch_root_hex else b\"\\x00\"*32\n",
        "xai_cid = norm_ipfs(cid)  # handles None, raw CID, or ipfs://CID\n",
        "rcpt = send_tx(phase3.functions.storeContentHash, model_id_bytes, content_hash, batch_root_b32, xai_cid)\n",
        "print(\"storeContentHash tx:\", rcpt.transactionHash.hex())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkaAaJGXdWH2"
      },
      "source": [
        "**Verify later**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RRg1fcmaMX6"
      },
      "outputs": [],
      "source": [
        "# === PHASE 3 — verify a content string against the chain (via event logs) ===\n",
        "from web3 import Web3\n",
        "from eth_abi import decode as abi_decode\n",
        "\n",
        "def verify_output_onchain(raw_text: str, model_id_hint: bytes = None, from_block=0, to_block=\"latest\"):\n",
        "    h = Web3.keccak(text=raw_text)  # bytes32\n",
        "    print(\"contentHash:\", h.hex())\n",
        "\n",
        "    # Try ABI-decoded event first (simplest)\n",
        "    try:\n",
        "        ev = phase3.events.ContentStored()\n",
        "        arg_filters = {\"contentHash\": h}\n",
        "        if model_id_hint is not None:\n",
        "            arg_filters[\"modelId\"] = model_id_hint\n",
        "        logs = ev.get_logs(fromBlock=from_block, toBlock=to_block, argument_filters=arg_filters)\n",
        "        if not logs:\n",
        "            print(\"Exists: False\")\n",
        "            return False\n",
        "\n",
        "        a = logs[-1][\"args\"]\n",
        "        print(\"Exists   :\", True)\n",
        "        print(\"submitter:\", a.get(\"submitter\"))\n",
        "        print(\"modelId  :\", \"0x\" + a.get(\"modelId\").hex())\n",
        "        print(\"batchRoot:\", \"0x\" + a.get(\"batchRoot\").hex())\n",
        "        print(\"xaiCID   :\", a.get(\"xaiCID\"))\n",
        "        print(\"time     :\", a.get(\"timestamp\"))\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"ABI event path failed, falling back to raw log scan:\", e)\n",
        "\n",
        "    # Fallback: raw topic scan (works even if your local ABI is missing the event)\n",
        "    # event ContentStored(bytes32 indexed modelId, bytes32 indexed contentHash, bytes32 indexed batchRoot, address submitter, string xaiCID, uint64 timestamp)\n",
        "    topic0 = Web3.keccak(text=\"ContentStored(bytes32,bytes32,bytes32,address,string,uint64)\")\n",
        "    topics = [topic0, None, h, None]  # filter by contentHash only\n",
        "    if model_id_hint is not None:\n",
        "        topics[1] = model_id_hint  # stricter filter if you want\n",
        "\n",
        "    flt = {\n",
        "        \"fromBlock\": from_block,\n",
        "        \"toBlock\": to_block,\n",
        "        \"address\": phase3.address,\n",
        "        \"topics\": topics,\n",
        "    }\n",
        "    logs = w3.eth.get_logs(flt)\n",
        "    if not logs:\n",
        "        print(\"Exists: False\")\n",
        "        return False\n",
        "\n",
        "    lg = logs[-1]\n",
        "    # Indexed topics order: [topic0, modelId, contentHash, batchRoot]\n",
        "    model_id_b32 = bytes.fromhex(lg[\"topics\"][1].hex()[2:])\n",
        "    batch_root_b32 = bytes.fromhex(lg[\"topics\"][3].hex()[2:])\n",
        "\n",
        "    # Data encodes (address submitter, string xaiCID, uint64 timestamp)\n",
        "    data_bytes = bytes.fromhex(lg[\"data\"][2:])\n",
        "    submitter, xaiCID, ts = abi_decode([\"address\", \"string\", \"uint64\"], data_bytes)\n",
        "\n",
        "    print(\"Exists   :\", True)\n",
        "    print(\"submitter:\", submitter)\n",
        "    print(\"modelId  :\", \"0x\" + model_id_b32.hex())\n",
        "    print(\"batchRoot:\", \"0x\" + batch_root_b32.hex())\n",
        "    print(\"xaiCID   :\", xaiCID)\n",
        "    print(\"time     :\", ts)\n",
        "    return True\n",
        "\n",
        "# Usage:\n",
        "verify_output_onchain(output_text, model_id_hint=model_id_bytes, from_block=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRQArjo3fli_"
      },
      "source": [
        "**Build & sign an EIP-712 receipt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1Ti1NzQaF_o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Use the **model owner** key or an **allowed publisher** key\n",
        "os.environ[\"EOA_PRIV_KEY\"] = \"<insert_pk>\"   # preferred\n",
        "# or:\n",
        "# os.environ[\"PUB_PRIV_KEY\"] = \"0x<your_private_key_hex>\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVOgZqJEfvos"
      },
      "outputs": [],
      "source": [
        "#Sanity check before running the eip-712 cells\n",
        "assert all(k in globals() for k in [\"w3\",\"acct\",\"phase2\",\"model_id_bytes\"]), \"Run Phase-2 cells first\"\n",
        "print(\"leaf count:\", len(leaves) if \"leaves\" in globals() else \"no leaves\")\n",
        "print(\"batch root:\", batch_root_hex if \"batch_root_hex\" in globals() else \"missing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoS4kM7waMaL"
      },
      "outputs": [],
      "source": [
        "# === PHASE 3 — EIP-712 receipt signer (publisher = your EOA) ===\n",
        "import os, json, time\n",
        "from eth_account import Account\n",
        "from eth_account.messages import encode_structured_data\n",
        "from web3 import Web3\n",
        "\n",
        "# --- helpers ---\n",
        "def _hex32(x: str | bytes | None) -> str:\n",
        "    \"\"\"Return 0x-prefixed 32-byte hex string or '0x' * 66 zero if None/empty.\"\"\"\n",
        "    if x is None:\n",
        "        return \"0x\" + \"00\"*32\n",
        "    if isinstance(x, (bytes, bytearray)):\n",
        "        h = x.hex()\n",
        "        if not h.startswith(\"0x\"):\n",
        "            h = \"0x\" + h\n",
        "        return h if len(h) == 66 else (\"0x\" + h[2:].rjust(64, \"0\"))\n",
        "    if isinstance(x, str):\n",
        "        h = x if x.startswith(\"0x\") else \"0x\"+x\n",
        "        if h == \"0x\":\n",
        "            return \"0x\" + \"00\"*32\n",
        "        # strip possible double 0x (seen before)\n",
        "        if h.startswith(\"0x0x\"):\n",
        "            h = \"0x\" + h[4:]\n",
        "        return h if len(h) == 66 else (\"0x\" + h[2:].rjust(64, \"0\"))\n",
        "    raise TypeError(\"unsupported type for _hex32\")\n",
        "\n",
        "def _to_bytes32(hx: str) -> bytes:\n",
        "    hx = _hex32(hx)\n",
        "    return bytes.fromhex(hx[2:])\n",
        "\n",
        "def _keccak_text(s: str) -> bytes:\n",
        "    return Web3.keccak(text=s)\n",
        "\n",
        "# --- publisher key (use the same EOA that owns the model, or an allowed publisher) ---\n",
        "PRIVATE_KEY = os.environ.get(\"EOA_PRIV_KEY\") or os.environ.get(\"PUB_PRIV_KEY\")\n",
        "assert PRIVATE_KEY, \"Set EOA_PRIV_KEY (or PUB_PRIV_KEY) in env\"\n",
        "if not PRIVATE_KEY.startswith(\"0x\"):\n",
        "    PRIVATE_KEY = \"0x\" + PRIVATE_KEY\n",
        "acct = Account.from_key(PRIVATE_KEY)\n",
        "publisher = acct.address\n",
        "\n",
        "# --- chain + contract addrs ---\n",
        "chain_id       = w3.eth.chain_id\n",
        "verifying_addr = phase3.address          # IMPORTANT: OutputAuthentication, not Phase-2\n",
        "registry_addr  = phase2.address          # FYI: used for provenance display only\n",
        "\n",
        "# --- gather Phase-2 bindings we already discovered earlier ---\n",
        "MODEL_ID_HEX   = _hex32(model_id_bytes)                                     # bytes32\n",
        "WEIGHTS_HEX    = _hex32(globals().get(\"WEIGHTS_HASH\") or globals().get(\"weights_hash\"))\n",
        "BATCH_ROOT_HEX = _hex32(globals().get(\"batch_root_hex\"))\n",
        "# ✅ Normalize batch CID to avoid ipfs://ipfs://... and accept raw CIDs too\n",
        "raw_cid = globals().get(\"batch_cid\") or \"\"\n",
        "BATCH_CID_STR = norm_ipfs(raw_cid)  # requires norm_ipfs(...) helper from the shared helpers cell\n",
        "\n",
        "# --- compute a leaf for THIS output (unsalted; fine for demo and signature path) ---\n",
        "# If you want reveal/non-reveal salts to match your Phase-2 batch exactly, replace these three lines\n",
        "# with the salted hashes you used when you built/committed the batch.\n",
        "input_hash  = _keccak_text(prompt_text)           # bytes32\n",
        "output_hash = _keccak_text(output_text)           # bytes32\n",
        "xai_hash    = b\"\\x00\"*32                          # or Web3.keccak(text=json.dumps(xai_obj, separators=(',',':')))\n",
        "leaf_bytes  = Web3.solidity_keccak(\n",
        "    [\"bytes32\",\"bytes32\",\"bytes32\"],\n",
        "    [input_hash, output_hash, xai_hash]\n",
        ")\n",
        "LEAF_HEX = _hex32(leaf_bytes)\n",
        "\n",
        "# --- tx hash for the batch (optional); zero if unknown here ---\n",
        "TX_HASH_HEX = _hex32(globals().get(\"rcpt_batch\").transactionHash.hex() if \"rcpt_batch\" in globals() else None)\n",
        "\n",
        "# --- build the typed data (must match Phase-3's RECEIPT_TYPEHASH) ---\n",
        "typed_data = {\n",
        "  \"domain\": {\n",
        "    \"name\": \"LLMOutputReceipt\",\n",
        "    \"version\": \"1\",\n",
        "    \"chainId\": chain_id,\n",
        "    \"verifyingContract\": verifying_addr,   # MUST be phase3.address\n",
        "  },\n",
        "  \"primaryType\": \"Receipt\",\n",
        "  \"types\": {\n",
        "    \"EIP712Domain\": [\n",
        "      {\"name\":\"name\",\"type\":\"string\"},\n",
        "      {\"name\":\"version\",\"type\":\"string\"},\n",
        "      {\"name\":\"chainId\",\"type\":\"uint256\"},\n",
        "      {\"name\":\"verifyingContract\",\"type\":\"address\"}\n",
        "    ],\n",
        "    \"Receipt\": [\n",
        "      {\"name\":\"modelId\",\"type\":\"bytes32\"},\n",
        "      {\"name\":\"weightsHash\",\"type\":\"bytes32\"},\n",
        "      {\"name\":\"batchRoot\",\"type\":\"bytes32\"},\n",
        "      {\"name\":\"batchCID\",\"type\":\"string\"},\n",
        "      {\"name\":\"txHash\",\"type\":\"bytes32\"},\n",
        "      {\"name\":\"index\",\"type\":\"uint256\"},\n",
        "      {\"name\":\"leaf\",\"type\":\"bytes32\"},\n",
        "      {\"name\":\"publisher\",\"type\":\"address\"},\n",
        "      {\"name\":\"timestamp\",\"type\":\"uint64\"}\n",
        "    ]\n",
        "  },\n",
        "  \"message\": {\n",
        "    \"modelId\":    _to_bytes32(MODEL_ID_HEX),\n",
        "    \"weightsHash\":_to_bytes32(WEIGHTS_HEX),\n",
        "    \"batchRoot\":  _to_bytes32(BATCH_ROOT_HEX),\n",
        "    \"batchCID\":   BATCH_CID_STR or \"\",            # string hashed per EIP-712 in the contract\n",
        "    \"txHash\":     _to_bytes32(TX_HASH_HEX),\n",
        "    \"index\":      int(0),                         # set the actual leaf index if you have it\n",
        "    \"leaf\":       _to_bytes32(LEAF_HEX),\n",
        "    \"publisher\":  publisher,\n",
        "    \"timestamp\":  int(time.time()),\n",
        "  }\n",
        "}\n",
        "\n",
        "msg = encode_structured_data(primitive=typed_data)\n",
        "sig = Account.sign_message(msg, private_key=PRIVATE_KEY)\n",
        "\n",
        "receipt = {\n",
        "    \"schema\": \"llm.receipt.v1\",\n",
        "    \"mode\": \"non-reveal\",             # or \"reveal\"\n",
        "    \"chainId\": chain_id,\n",
        "    \"registry\": registry_addr,        # for human readers; NOT in EIP-712 domain\n",
        "    \"modelId\": MODEL_ID_HEX,\n",
        "    \"weightsHash\": WEIGHTS_HEX,\n",
        "    \"batchRoot\": BATCH_ROOT_HEX,\n",
        "    \"batchCID\": BATCH_CID_STR or \"\",\n",
        "    \"txHash\": TX_HASH_HEX,\n",
        "    \"index\": 0,\n",
        "    \"leaf\": LEAF_HEX,\n",
        "    \"publisher\": publisher,\n",
        "    \"timestamp\": typed_data[\"message\"][\"timestamp\"],\n",
        "    \"eip712\": {\n",
        "        \"domain\": typed_data[\"domain\"],\n",
        "        \"types\":  typed_data[\"types\"],\n",
        "        \"message\": {\n",
        "            # serialize back to hex strings for the JSON blob\n",
        "            \"modelId\": MODEL_ID_HEX,\n",
        "            \"weightsHash\": WEIGHTS_HEX,\n",
        "            \"batchRoot\": BATCH_ROOT_HEX,\n",
        "            \"batchCID\": typed_data[\"message\"][\"batchCID\"],\n",
        "            \"txHash\": TX_HASH_HEX,\n",
        "            \"index\": typed_data[\"message\"][\"index\"],\n",
        "            \"leaf\": LEAF_HEX,\n",
        "            \"publisher\": publisher,\n",
        "            \"timestamp\": typed_data[\"message\"][\"timestamp\"],\n",
        "        },\n",
        "        \"signature\": sig.signature.hex(),\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Signed receipt for index\", receipt[\"index\"])\n",
        "print(\"publisher:\", publisher)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6KAvc7faMcu"
      },
      "outputs": [],
      "source": [
        "# === PHASE 3 — verify EIP-712 receipt (off-chain) + optional on-chain cross-check ===\n",
        "from eth_account.messages import encode_typed_data  # use this (new) instead of encode_structured_data\n",
        "from eth_account import Account\n",
        "from web3 import Web3\n",
        "\n",
        "def _to_b32(hx: str) -> bytes:\n",
        "    \"\"\"Convert 0x-prefixed 32-byte hex to bytes32; returns zero32 if '0x'.\"\"\"\n",
        "    if not isinstance(hx, str):\n",
        "        raise TypeError(\"expected hex string\")\n",
        "    if hx == \"0x\":\n",
        "        return b\"\\x00\" * 32\n",
        "    if not hx.startswith(\"0x\"):\n",
        "        hx = \"0x\" + hx\n",
        "    return bytes.fromhex(hx[2:])\n",
        "\n",
        "def verify_receipt(receipt: dict):\n",
        "    # Rebuild typed data from the JSON blob\n",
        "    td = {\n",
        "        \"domain\": receipt[\"eip712\"][\"domain\"],\n",
        "        \"primaryType\": \"Receipt\",\n",
        "        \"types\": receipt[\"eip712\"][\"types\"],\n",
        "        \"message\": receipt[\"eip712\"][\"message\"].copy(),  # shallow copy, we'll mutate types\n",
        "    }\n",
        "\n",
        "    # Convert hex strings to bytes for all bytes32 fields (required by encoder)\n",
        "    for k in (\"modelId\", \"weightsHash\", \"batchRoot\", \"txHash\", \"leaf\"):\n",
        "        td[\"message\"][k] = _to_b32(td[\"message\"][k])\n",
        "\n",
        "    # The other fields already have the correct python types:\n",
        "    # - batchCID: str\n",
        "    # - index: int\n",
        "    # - publisher: \"0x...\" address (string is OK)\n",
        "    # - timestamp: int\n",
        "\n",
        "    # Encode per EIP-712 and recover signer\n",
        "    signable = encode_typed_data(full_message=td)\n",
        "    sig_hex = receipt[\"eip712\"][\"signature\"]\n",
        "    sig_bytes = bytes.fromhex(sig_hex[2:] if sig_hex.startswith(\"0x\") else sig_hex)\n",
        "    signer = Account.recover_message(signable, signature=sig_bytes)\n",
        "    ok_sig = (signer.lower() == receipt[\"publisher\"].lower())\n",
        "\n",
        "    result = {\"ok_signature\": ok_sig, \"signer\": signer}\n",
        "\n",
        "    # ---- Optional: cross-check with the Phase-3 contract's verifyReceipt ----\n",
        "    try:\n",
        "        r_msg = receipt[\"eip712\"][\"message\"]\n",
        "        r_tuple = (\n",
        "            _to_b32(receipt[\"modelId\"]),\n",
        "            _to_b32(receipt[\"weightsHash\"]),\n",
        "            _to_b32(receipt[\"batchRoot\"]),\n",
        "            r_msg[\"batchCID\"],\n",
        "            _to_b32(receipt[\"txHash\"]),\n",
        "            int(r_msg[\"index\"]),\n",
        "            _to_b32(receipt[\"leaf\"]),\n",
        "            receipt[\"publisher\"],\n",
        "            int(r_msg[\"timestamp\"]),\n",
        "        )\n",
        "        ok_onchain, signer_onchain = phase3.functions.verifyReceipt(r_tuple, sig_bytes).call()\n",
        "        result.update({\"ok_onchain\": ok_onchain, \"signer_onchain\": signer_onchain})\n",
        "    except Exception as e:\n",
        "        result.update({\"ok_onchain\": None, \"onchain_error\": str(e)})\n",
        "\n",
        "    return result\n",
        "\n",
        "verify_receipt(receipt)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
